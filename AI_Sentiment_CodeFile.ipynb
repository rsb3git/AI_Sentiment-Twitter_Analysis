{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsb3git/AI_Sentiment-Twitter_Analysis/blob/main/AI_Sentiment_CodeFile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install and Import"
      ],
      "metadata": {
        "id": "rEfw1NO_F6K8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/zlisto/social_media_analytics\n",
        "\n",
        "import os\n",
        "os.chdir(\"social_media_analytics\")"
      ],
      "metadata": {
        "id": "wHaEii6PLTak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt --quiet"
      ],
      "metadata": {
        "id": "q3-n54jmOnQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "import gensim.downloader as api\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "\n",
        "import sklearn.cluster as cluster\n",
        "from sklearn import metrics\n",
        "from scipy import stats\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.lda_model\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "import scripts.TextAnalysis as ta\n",
        "from scripts.api import *"
      ],
      "metadata": {
        "id": "iWdkCBISPO-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "F-HHpndmYANc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import codecs  #this let's us display tweets properly (emojis, etc.)\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "metadata": {
        "id": "luoyiRz3YKh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"
      ],
      "metadata": {
        "id": "i7PukQItYW6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting Drive"
      ],
      "metadata": {
        "id": "cvpjCknbf3QH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for mounting and unmounting your Google drive so you can access Arpita's DB files"
      ],
      "metadata": {
        "id": "yIirx47XPB8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "piHdybikIpka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drive.flush_and_unmount()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQh7LvesOxD2",
        "outputId": "4d39afba-c668-4557-9689-5285fe7a7301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After mounting your drive, run this code to access the DB"
      ],
      "metadata": {
        "id": "jZ_TAboZPOax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fname_db = '/content/drive/MyDrive/Social Media Analytics/AItweets_v2'\n",
        "df = DB.fetch(table_name='keyword_tweets',path=fname_db)\n",
        "n=len(df)\n",
        "print(f\"{n} tweets\")\n",
        "df.head(n=2)"
      ],
      "metadata": {
        "id": "L3X5PUlKL65R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CLEAN TEXT DATA**"
      ],
      "metadata": {
        "id": "Vxhin8X1V9qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#clean text data\n",
        "\n",
        "df['text_clean'] = df.text.apply(ta.clean_tweet)  #clean the tweets\n",
        "df = df[df.text_clean.str.len() >0]  #remove cleaned tweets of length 0\n",
        "nclean = len(df)\n",
        "print(f\"{n} tweets, {nclean} clean tweets\")\n",
        "\n",
        "df.sample(n=5)"
      ],
      "metadata": {
        "id": "m9CtHofdV1ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wordclouds, frequency, likes, retweets, tweet rate"
      ],
      "metadata": {
        "id": "UPsUYkIfLgdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Wordcloud around keyword tweets"
      ],
      "metadata": {
        "id": "XgG2dnxFgneo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add Created At Datetime Column and Sort\n",
        "format = \"%Y-%m-%d %H:%M:%S\"\n",
        "df['created_at_datetime'] = pd.to_datetime(df['created_at'],format=format).dt.tz_localize(None)\n",
        "df.sort_values(by = 'created_at_datetime',\n",
        "               inplace = True,\n",
        "               ascending = True)\n",
        "\n",
        "#define initial and final time variables (for plot axis limits)\n",
        "ti = df.created_at_datetime.head(n=1).values[0]\n",
        "tf = df.created_at_datetime.tail(n=1).values[0]\n",
        "\n",
        "print(f\"Head: {ti}\")\n",
        "print(f\"Tail: {tf}\")"
      ],
      "metadata": {
        "id": "uT_5EidLgm7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(STOPWORDS)\n",
        "stopwords.add(\"de\")\n",
        "stopwords.add(\"que\")\n",
        "stopwords.add(\"chatgpt\")\n",
        "stopwords.add(\"ai\")\n",
        "stopwords.add('artificial intelligence')\n",
        "stopwords.add(\"gpt4\")\n",
        "stopwords.add(\"gpt-4\")\n",
        "stopwords.add(\"midjourney\")\n",
        "stopwords.add(\"dalle\")\n",
        "stopwords.add(\"dall-e\")\n",
        "text=' '.join(df.text_clean.tolist()).lower()\n",
        "wordcloud = WordCloud(stopwords=stopwords,max_font_size=150,\n",
        "                      max_words=100,\n",
        "                      background_color=\"black\",\n",
        "                      width=1000,\n",
        "                      height=600)\n",
        "\n",
        "wordcloud.generate(text)\n",
        "\n",
        "#visualize word cloud\n",
        "fig = plt.figure(figsize = (10,8))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_w595bfSgtRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Keyword tweets vs like count and retweet count"
      ],
      "metadata": {
        "id": "7OPPYusGg1NP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#keywords and tweets graph\n",
        "\n",
        "keywords = ['AI' or '\"Artificial Intelligence\"','Dall-E' or 'DallE',\n",
        "            'ChatGPT', 'Midjourney', 'GPT-4' or 'GPT4']\n",
        "\n",
        "df['keyword'] = None\n",
        "for keyword in keywords:\n",
        "    ind =  df.text.str.contains(keyword, case=False)\n",
        "    df[f'keyword_{keyword}'] = ind\n",
        "    df.loc[ind, 'keyword'] = keyword\n",
        "    print(f\"{keyword}: {len(df[df[f'keyword_{keyword}']==True])} tweets \")\n",
        "\n",
        "print(f\"{len(df)} total tweets\")\n",
        "\n",
        "fig = plt.figure(figsize = (16,6))\n",
        "ax1 = plt.subplot(1,2,1)\n",
        "sns.barplot(data = df,\n",
        "                x = 'keyword',\n",
        "                y = 'like_count',\n",
        "                color = 'darkorange')\n",
        "plt.ylabel(\"Like Count\")\n",
        "plt.xlabel(\"Keyword\")\n",
        "plt.title(\"Keywords vs Like Count\",fontsize = 20)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize = (16,6))\n",
        "ax2 = plt.subplot(1,2,2)\n",
        "sns.barplot(data = df,\n",
        "                x = 'keyword',\n",
        "                y = 'retweet_count',\n",
        "                color = 'blue')\n",
        "plt.ylabel(\"Retweet Count\")\n",
        "plt.xlabel(\"Keyword\")\n",
        "plt.title(\"Keywords vs Retweet Count\",fontsize = 20)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MWjbErVgg0ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Keyword frequency"
      ],
      "metadata": {
        "id": "lm6VpDBtui8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#keyword frequency\n",
        "\n",
        "for keyword in keywords:\n",
        "    ind =  df.text.str.contains(keyword, case=False)\n",
        "    df.loc[ind, 'keyword'] = keyword\n",
        "ax = sns.countplot(data = df,\n",
        "                 x = 'keyword')\n",
        "plt.grid()\n",
        "plt.xlabel(\"Keyword\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Keyword Frequency\",fontsize = 20)\n",
        "ax.set_xticklabels(ax.get_xticklabels(),\n",
        "                     rotation = 90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_oXFz6rYuZtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keyword tweet rate"
      ],
      "metadata": {
        "id": "5suSqEmYhDs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#keyword tweet rate\n",
        "colors = ['red','blue','orange','green','cyan']\n",
        "fig = plt.figure(figsize = (12,4))\n",
        "\n",
        "for color,keyword in zip(colors,keywords):\n",
        "  df_plot = df[df.text.str.contains(keyword, case=False)].copy()\n",
        "  df_plot['tweet_indicator'] = np.ones(len(df_plot))\n",
        "  df_plot[f'rate_1D_{keyword}'] = df_plot.rolling('1D',on = 'created_at_datetime')['tweet_indicator'].sum()\n",
        "\n",
        "  sns.lineplot(data = df_plot,\n",
        "               x = 'created_at_datetime',\n",
        "               y = f'rate_1D_{keyword}',\n",
        "               label = keyword,linewidth = 1,\n",
        "               color = color)\n",
        "\n",
        "plt.grid()\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Rate [tweets/day]\")\n",
        "plt.xlim(ti,tf)\n",
        "#plt.ylim([0,30])\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t73VkvkKhF0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Peak rate and tweets"
      ],
      "metadata": {
        "id": "rfnrqbt2hGqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#peak tweet rate and tweets at peak\n",
        "\n",
        "df['tweet_indicator'] = np.ones(len(df))\n",
        "df['rate_1D'] = df.rolling('1D',on = 'created_at_datetime').sum()['tweet_indicator']\n",
        "df['rate_7D'] = df.rolling('7D',on = 'created_at_datetime').sum()['tweet_indicator']/7\n",
        "\n",
        "tpeak = df[df.rate_1D==df.rate_1D.max()].created_at_datetime.values[0]\n",
        "print(f\"Peak tweet rate on {tpeak}\")\n",
        "\n",
        "t0 = tpeak - np.timedelta64(24, 'h')\n",
        "t1 = tpeak\n",
        "df1 = df[(df.created_at_datetime>=t0) & (df.created_at_datetime<t1)]\n",
        "df1[['text']]"
      ],
      "metadata": {
        "id": "XCO0iuNjhK0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wordcloud of peak tweets\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.add(\"de\")\n",
        "stopwords.add(\"que\")\n",
        "stopwords.add(\"chatgpt\")\n",
        "stopwords.add(\"ai\")\n",
        "stopwords.add('\"artificial intelligence\"')\n",
        "stopwords.add(\"gpt4\")\n",
        "stopwords.add(\"gpt-4\")\n",
        "stopwords.add(\"midjourney\")\n",
        "stopwords.add(\"dalle\")\n",
        "stopwords.add(\"dall-e\")\n",
        "text=' '.join(df1.text_clean.tolist()).lower()\n",
        "wordcloud = WordCloud(stopwords=stopwords,max_font_size=150, max_words=100, background_color=\"white\",width=1000, height=600)\n",
        "wordcloud.generate(text)\n",
        "\n",
        "fig = plt.figure(figsize = (10,10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6GvGllg_hNjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top retweeted tweets"
      ],
      "metadata": {
        "id": "u4w4wqdFhS0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ndisplay = 10  #number of tweets to display\n",
        "\n",
        "df_u = df[df.text.str.contains(\"RT @\", case=False)==False] #remove any retweets\n",
        "df.sort_values(by = 'retweet_count', ascending = False, inplace = True)\n",
        "print(\"\\n\")\n",
        "display(df[['retweet_count','text']].head(n=ndisplay))"
      ],
      "metadata": {
        "id": "f7nD6pOUhQUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Top words and wordcloud"
      ],
      "metadata": {
        "id": "YLCOk4F-hZ9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(STOPWORDS)  #set the stopwords\n",
        "stopwords.add(\"de\")\n",
        "stopwords.add(\"que\")\n",
        "stopwords.add(\"chatgpt\")\n",
        "stopwords.add(\"ai\")\n",
        "stopwords.add('\"artificial intelligence\"')\n",
        "stopwords.add(\"gpt4\")\n",
        "stopwords.add(\"gpt-4\")\n",
        "stopwords.add(\"midjourney\")\n",
        "stopwords.add(\"dalle\")\n",
        "stopwords.add(\"dall-e\")\n",
        "\n",
        "words_max = 10 #maximum number of words to plot in word frequency plot\n",
        "\n",
        "\n",
        "\n",
        "text=' '.join(df_u.text_clean.tolist()).lower()\n",
        "\n",
        "#generate word cloud\n",
        "wordcloud = WordCloud(stopwords=stopwords,\n",
        "                      max_font_size=150,\n",
        "                      max_words=100,\n",
        "                      background_color=\"black\",\n",
        "                      width=1000,\n",
        "                      height=600)\n",
        "wordcloud.generate(text)\n",
        "\n",
        "#create dataframe of words and frequencies\n",
        "df_words = pd.DataFrame({'word':wordcloud.words_.keys(),\n",
        "                          'frequency':wordcloud.words_.values()})\n",
        "df_words = df_words.sort_values(by = 'frequency',\n",
        "                                ascending = False)\n",
        "\n",
        "#plot word cloud and word frequencies\n",
        "plt.figure(figsize = (16,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.title(\"Top words\", fontsize = 20)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "ax= sns.barplot(data = df_words[0:words_max],\n",
        "                x = 'word',\n",
        "                y = 'frequency',\n",
        "                color = 'orange')\n",
        "ax.set_xticklabels(ax.get_xticklabels(),\n",
        "                    rotation = 90)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JIyOtkWBhb94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding and Clustering"
      ],
      "metadata": {
        "id": "6ZH3truSLOT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF Embedding"
      ],
      "metadata": {
        "id": "pE6NYZpVhc7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tf embedding\n",
        "tf_vectorizer = CountVectorizer(min_df=5, stop_words='english')\n",
        "tf_embedding = tf_vectorizer.fit_transform(df.text_clean)\n",
        "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
        "\n",
        "nvocab = len(tf_feature_names)\n",
        "ntweets = len(df.text_clean)\n",
        "print(f\"{ntweets} tweets, {nvocab} words in vocabulary\")\n",
        "print(f\"TF embedding shape is {tf_embedding.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LJW4-W8hhMr",
        "outputId": "fc7365b2-f863-47f3-dcd7-864c6841a6cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33543 tweets, 8858 words in vocabulary\n",
            "TF embedding shape is (33543, 8858)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF Embedding"
      ],
      "metadata": {
        "id": "_bQXq36KhizN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tf-idf embedding\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=5, stop_words='english')\n",
        "tfidf_embedding = tfidf_vectorizer.fit_transform(df.text_clean)\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "nvocab = len(tfidf_feature_names)\n",
        "print(f\"{ntweets} tweets, {nvocab} words in vocabulary\")\n",
        "print(f\"TF-IDF embedding shape is {tfidf_embedding.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpuTv-cPhmZB",
        "outputId": "99a2a3f4-0d85-445c-e416-4e16bfc5606f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33543 tweets, 8858 words in vocabulary\n",
            "TF-IDF embedding shape is (33543, 8858)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA Embedding"
      ],
      "metadata": {
        "id": "IJf7njwDhome"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lda embedding\n",
        "%%time\n",
        "num_topics = 5\n",
        "lda = LatentDirichletAllocation(n_components=num_topics, max_iter=5,\n",
        "                                learning_method='online', learning_offset=50.,\n",
        "                                random_state=0).fit(tf_embedding)\n",
        "lda_embedding = lda.transform(tf_embedding)\n",
        "print(f\"{ntweets} tweets, {num_topics} topics in LDA model\")\n",
        "print(f\"shape of lda embedding is {lda_embedding.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGIKDpfDhr05",
        "outputId": "d83e346a-f2ca-4f38-f184-ccd5509903ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33543 tweets, 5 topics in LDA model\n",
            "shape of lda embedding is (33543, 5)\n",
            "CPU times: user 35.3 s, sys: 121 ms, total: 35.4 s\n",
            "Wall time: 35.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "viz = pyLDAvis.lda_model.prepare(lda, tf_embedding, tf_vectorizer)\n",
        "pyLDAvis.display(viz)"
      ],
      "metadata": {
        "id": "bf39_xBPhtyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Umap Embedding"
      ],
      "metadata": {
        "id": "h8UeIkGMhynM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#umap\n",
        "umap_tf_embedding = umap.UMAP(n_components=2, metric='hellinger').fit_transform(tf_embedding)\n",
        "umap_tfidf_embedding = umap.UMAP(n_components=2, metric='hellinger').fit_transform(tfidf_embedding)\n",
        "\n",
        "df['tf_umap_x'] = umap_tf_embedding[:,0]\n",
        "df['tf_umap_y'] = umap_tf_embedding[:,1]\n",
        "df['tfidf_umap_x'] = umap_tfidf_embedding[:,0]\n",
        "df['tfidf_umap_y'] = umap_tfidf_embedding[:,1]\n",
        "#zscoring centers the vectors at zero\n",
        "umap_tf_embedding = stats.zscore(umap_tf_embedding,nan_policy='omit')\n",
        "umap_tfidf_embedding = stats.zscore(umap_tfidf_embedding,nan_policy='omit')\n",
        "\n",
        "xmax = 3  #range for x-axis\n",
        "ymax = 3  #range for y-axis\n",
        "s = 5  #marker size\n",
        "\n",
        "fig = plt.figure(figsize = (16,8))\n",
        "\n",
        "ax1 = plt.subplot(1,2,1)\n",
        "sns.scatterplot(data=df, x=\"tf_umap_x\",\n",
        "                y=\"tf_umap_y\", hue=\"screen_name\", s=s)\n",
        "plt.title(\"TF Embedding\")\n",
        "plt.xlim([-xmax, xmax])\n",
        "plt.ylim([-ymax,ymax])\n",
        "\n",
        "ax2 = plt.subplot(1,2,2)\n",
        "sns.scatterplot(data=df, x=\"tfidf_umap_x\",\n",
        "                y=\"tfidf_umap_y\", hue=\"screen_name\", s=s)\n",
        "plt.title(\"TF-IDF Embedding\");\n",
        "plt.xlim([-xmax, xmax])\n",
        "plt.ylim([-ymax,ymax])\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Cm-FRqqgh1Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above umap code gave us issues because of quirks in our data"
      ],
      "metadata": {
        "id": "si4zJnMbh8SQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-means Clustering"
      ],
      "metadata": {
        "id": "KbvYbc98pouW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cluster tweets using kmeans\n",
        "\n",
        "n_clusters = 3\n",
        "\n",
        "kmeans_label = cluster.KMeans(n_clusters=n_clusters).fit_predict(tf_embedding)\n",
        "df['kmeans_label_tf'] = [str(x) for x in kmeans_label]\n",
        "\n",
        "kmeans_label = cluster.KMeans(n_clusters=n_clusters).fit_predict(tfidf_embedding)\n",
        "df['kmeans_label_tfidf'] = [str(x) for x in kmeans_label]\n",
        "\n",
        "kmeans_label = cluster.KMeans(n_clusters=n_clusters).fit_predict(lda_embedding)\n",
        "df['kmeans_label_lda'] = [str(x) for x in kmeans_label]\n"
      ],
      "metadata": {
        "id": "B5yNDlhairEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wordcloud\n",
        "def kmeans_wordcloud(df, cluster_label_column,stopwords):\n",
        "    print(cluster_label_column)\n",
        "    for k in np.sort(df[cluster_label_column].unique()):\n",
        "        s=df[df[cluster_label_column]==k]\n",
        "        text=' '.join(s.text_clean.tolist()).lower()\n",
        "        wordcloud = WordCloud(stopwords=stopwords,max_font_size=150, max_words=100, background_color=\"white\",width=1000, height=600)\n",
        "        wordcloud.generate(text)\n",
        "        print(f\"\\n\\tCluster {k} {cluster_label_column} has {len(s)} tweets\")\n",
        "      #  plt.subplot(1,2,2)\n",
        "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "    return 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOV8NT6bptb2",
        "outputId": "388bdfb0-389f-4ae4-adb6-e53e8ff77a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wordclouds for tf\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.add(\"chatgpt\")\n",
        "stopwords.add(\"ai\")\n",
        "stopwords.add('\"artificial intelligence\"')\n",
        "stopwords.add(\"gpt4\")\n",
        "stopwords.add(\"gpt-4\")\n",
        "stopwords.add(\"midjourney\")\n",
        "stopwords.add(\"dalle\")\n",
        "stopwords.add(\"dall-e\")\n",
        "cluster_label_column= 'kmeans_label_tf'\n",
        "kmeans_wordcloud(df,cluster_label_column,stopwords)"
      ],
      "metadata": {
        "id": "7iuE6jdVpvbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wordclouds for tfidf\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.add(\"chatgpt\")\n",
        "stopwords.add(\"ai\")\n",
        "stopwords.add('\"artificial intelligence\"')\n",
        "stopwords.add(\"gpt4\")\n",
        "stopwords.add(\"gpt-4\")\n",
        "stopwords.add(\"midjourney\")\n",
        "stopwords.add(\"dalle\")\n",
        "stopwords.add(\"dall-e\")\n",
        "cluster_label_column= 'kmeans_label_tfidf'\n",
        "kmeans_wordcloud(df,cluster_label_column,stopwords)"
      ],
      "metadata": {
        "id": "EpH1e064pybJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wordclouds for lda\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.add(\"chatgpt\")\n",
        "stopwords.add(\"ai\")\n",
        "stopwords.add('\"artificial intelligence\"')\n",
        "stopwords.add(\"gpt4\")\n",
        "stopwords.add(\"gpt-4\")\n",
        "stopwords.add(\"midjourney\")\n",
        "stopwords.add(\"dalle\")\n",
        "stopwords.add(\"dall-e\")\n",
        "cluster_label_column= 'kmeans_label_lda'\n",
        "kmeans_wordcloud(df,cluster_label_column,stopwords )"
      ],
      "metadata": {
        "id": "f0zjo-s3pzEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname_db = '../drive/MyDrive/historic_v2'\n",
        "df_tweets = DB.fetch(table_name='keyword_tweets',path=fname_db)\n",
        "df_tweets.head()"
      ],
      "metadata": {
        "id": "peqXJrUozEXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MH code for average sentiment over time - this code cleans and preps\n",
        "\n",
        "#need to access the database first....historic_v2.db\n",
        "\n",
        "df_tweets['text_clean'] = df_tweets.text.apply(ta.clean_tweet)  #clean the tweets\n",
        "df_tweets = df_tweets[df_tweets.text_clean.str.len() >0]  #remove cleaned tweets of length 0\n",
        "nclean = len(df_tweets)\n",
        "\n",
        "df_tweets.sample(n=5)\n",
        "\n",
        "def sentiment_classifier(text,model,tokenizer):\n",
        "    inputs = tokenizer.encode_plus(text, return_tensors='pt', add_special_tokens=True)\n",
        "\n",
        "    token_type_ids = inputs['token_type_ids']\n",
        "    input_ids = inputs['input_ids']\n",
        "\n",
        "    output = model(input_ids, token_type_ids=token_type_ids,return_dict=True,output_hidden_states=True)\n",
        "    logits = np.array(output.logits.tolist()[0])\n",
        "    prob = np.exp(logits)/np.sum(np.exp(logits))\n",
        "    sentiment = np.sum([(x+1)*prob[x] for x in range(len(prob))])  #use this line if you want the mean score\n",
        "    embedding = output.hidden_states[12].detach().numpy().squeeze()[0]\n",
        "\n",
        "    return sentiment,embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8eDYJOip8E6",
        "outputId": "5fb20f89-76dc-4801-f531-47d128e637a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "c = 0\n",
        "Sentiment = []\n",
        "Embedding = []\n",
        "for index,row in df_tweets.iterrows():  #iterate over rows of dataframe\n",
        "    c+=1\n",
        "    if c%1000==0:print(f\"Tweet {c}/{len(df_tweets)}\")  #print progres every 1000 rows\n",
        "\n",
        "    sentiment,embedding = sentiment_classifier(row.text,model,tokenizer)  #calculate sentiment and embedding of tweet\n",
        "    Sentiment.append(sentiment)  #append sentiment of tweet to Sentiment list\n",
        "    Embedding.append(embedding) #append embedding of tweet to Embedding list\n",
        "\n",
        "df_tweets['sentiment'] = Sentiment  #add sentiment column to dataframe of tweets\n",
        "df_tweets.head()"
      ],
      "metadata": {
        "id": "8f_spz-otLUJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ddda1f5f-f51d-4713-803a-b407111fbf40"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet 1000/29633\n",
            "Tweet 2000/29633\n",
            "Tweet 3000/29633\n",
            "Tweet 4000/29633\n",
            "Tweet 5000/29633\n",
            "Tweet 6000/29633\n",
            "Tweet 7000/29633\n",
            "Tweet 8000/29633\n",
            "Tweet 9000/29633\n",
            "Tweet 10000/29633\n",
            "Tweet 11000/29633\n",
            "Tweet 12000/29633\n",
            "Tweet 13000/29633\n",
            "Tweet 14000/29633\n",
            "Tweet 15000/29633\n",
            "Tweet 16000/29633\n",
            "Tweet 17000/29633\n",
            "Tweet 18000/29633\n",
            "Tweet 19000/29633\n",
            "Tweet 20000/29633\n",
            "Tweet 21000/29633\n",
            "Tweet 22000/29633\n",
            "Tweet 23000/29633\n",
            "Tweet 24000/29633\n",
            "Tweet 25000/29633\n",
            "Tweet 26000/29633\n",
            "Tweet 27000/29633\n",
            "Tweet 28000/29633\n",
            "Tweet 29000/29633\n",
            "CPU times: user 2h 7min 27s, sys: 11.4 s, total: 2h 7min 39s\n",
            "Wall time: 2h 8min 25s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 created_at screen_name  \\\n",
              "0  2022-04-29T23:59:09.000Z        None   \n",
              "1  2022-04-29T23:56:12.000Z        None   \n",
              "2  2022-04-29T23:55:05.000Z        None   \n",
              "3  2022-04-29T23:53:40.000Z        None   \n",
              "4  2022-04-29T23:53:06.000Z        None   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                   text  \\\n",
              "0                                                                                                                                                                                         10 Reasons Why you Should Learn Artificial Intelligence https://t.co/nqYx4aS058 #deeplearning   \n",
              "1  Gods And Goddesses Gods, and Goddesses can range from artificial intelligence to Earthly nature deities that all ultimately fall under the umbrella of higher dimensional ... https://t.co/ODSA8XFEND #GodsGoddesses #NatureDeities #Supernatural #Paranormal #Fantasy #scifi #ASMSG   \n",
              "2             There’s no shortage of #data on customers these days. Companies are collecting reams of it to get the best insights on their customers.\\n\\nAI in #CustomerAnalytics - https://t.co/RmRMDPolFL \\n\\n#AI #Automation #TechUpdate #IT #tech #industry https://t.co/7QqKb3rS7Z   \n",
              "3                                                                                                                                        Economics Homework Help HSM Artificial Intelligence &amp; Internet of Things &amp; Its Impact on Delgro Worksheet\\n\\n  https://t.co/q658tYPOsh   \n",
              "4               Artificial intelligence challenges what it means to be creative - Science News Magazine: Artificial intelligence challenges what it means to be creative  Science News Magazine https://t.co/rGKIaITETH #AI #artificialintelligence #Finperform https://t.co/9cNPF8Ptyb   \n",
              "\n",
              "  lang  retweet_count  reply_count  like_count  quote_count  impression_count  \\\n",
              "0   en              0            0           0            0                 0   \n",
              "1   en              0            0           0            0                 0   \n",
              "2   en              0            0           0            0                 0   \n",
              "3   en              0            0           0            0                 0   \n",
              "4   en              0            0           0            0                 0   \n",
              "\n",
              "                    id            author_id      conversation_id  \\\n",
              "0  1520190984910479368  1013232376758128643  1520190984910479368   \n",
              "1  1520190245341261825            388385054  1520190245341261825   \n",
              "2  1520189963614277633  1334030877018316801  1520189963614277633   \n",
              "3  1520189606162956289  1353946694120067074  1520189606162956289   \n",
              "4  1520189463766265857   856240505826496513  1520189463766265857   \n",
              "\n",
              "  in_reply_to_user_id   geo  \\\n",
              "0                 nan  None   \n",
              "1                 nan  None   \n",
              "2                 nan  None   \n",
              "3                 nan  None   \n",
              "4                 nan  None   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   entities  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {'urls': [{'start': 56, 'end': 79, 'url': 'https://t.co/nqYx4aS058', 'expanded_url': 'https://hackernoon.com/10-reasons-why-you-should-learn-artificial-intelligence-5v6q30vo', 'display_url': 'hackernoon.com/10-reasons-why…', 'status': 200, 'unwound_url': 'https://hackernoon.com/10-reasons-why-you-should-learn-artificial-intelligence-5v6q30vo'}], 'hashtags': [{'start': 80, 'end': 93, 'tag': 'deeplearning'}]}   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {'urls': [{'start': 174, 'end': 197, 'url': 'https://t.co/ODSA8XFEND', 'expanded_url': 'https://mysticinvestigations.com/gods/', 'display_url': 'mysticinvestigations.com/gods/', 'status': 200, 'unwound_url': 'https://mysticinvestigations.com/gods/'}], 'annotations': [{'start': 271, 'end': 275, 'probability': 0.5446, 'type': 'Other', 'normalized_text': 'ASMSG'}], 'hashtags': [{'start': 198, 'end': 212, 'tag': 'GodsGoddesses'}, {'start': 213, 'end': 227, 'tag': 'NatureDeities'}, {'start': 228, 'end': 241, 'tag': 'Supernatural'}, {'start': 242, 'end': 253, 'tag': 'Paranormal'}, {'start': 254, 'end': 262, 'tag': 'Fantasy'}, {'start': 263, 'end': 269, 'tag': 'scifi'}, {'start': 270, 'end': 276, 'tag': 'ASMSG'}]}   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                       {'urls': [{'start': 164, 'end': 187, 'url': 'https://t.co/RmRMDPolFL', 'expanded_url': 'https://bit.ly/3vJ91SI', 'display_url': 'bit.ly/3vJ91SI', 'status': 200, 'unwound_url': 'https://www.itpro.co.uk/technology/artificial-intelligence-ai/361259/ai-in-customer-analytics'}, {'start': 238, 'end': 261, 'url': 'https://t.co/7QqKb3rS7Z', 'expanded_url': 'https://twitter.com/IgnitivOfficial/status/1520189963614277633/photo/1', 'display_url': 'pic.twitter.com/7QqKb3rS7Z', 'media_key': '3_1520189961424838657'}], 'annotations': [{'start': 137, 'end': 138, 'probability': 0.8035, 'type': 'Organization', 'normalized_text': 'AI'}, {'start': 191, 'end': 192, 'probability': 0.6384, 'type': 'Organization', 'normalized_text': 'AI'}], 'hashtags': [{'start': 23, 'end': 28, 'tag': 'data'}, {'start': 143, 'end': 161, 'tag': 'CustomerAnalytics'}, {'start': 190, 'end': 193, 'tag': 'AI'}, {'start': 194, 'end': 205, 'tag': 'Automation'}, {'start': 206, 'end': 217, 'tag': 'TechUpdate'}, {'start': 218, 'end': 221, 'tag': 'IT'}, {'start': 222, 'end': 227, 'tag': 'tech'}, {'start': 228, 'end': 237, 'tag': 'industry'}]}   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {'urls': [{'start': 117, 'end': 140, 'url': 'https://t.co/q658tYPOsh', 'expanded_url': 'https://besthomeworkservices.com/economics-homework-help-7780/?utm_source=ReviveOldPost&utm_medium=social&utm_campaign=ReviveOldPost', 'display_url': 'besthomeworkservices.com/economics-home…', 'status': 403, 'unwound_url': 'https://besthomeworkservices.com/economics-homework-help-7780/?utm_source=ReviveOldPost&utm_medium=social&utm_campaign=ReviveOldPost'}], 'annotations': [{'start': 24, 'end': 26, 'probability': 0.4441, 'type': 'Other', 'normalized_text': 'HSM'}, {'start': 97, 'end': 112, 'probability': 0.3944, 'type': 'Other', 'normalized_text': 'Delgro Worksheet'}]}   \n",
              "4  {'urls': [{'start': 176, 'end': 199, 'url': 'https://t.co/rGKIaITETH', 'expanded_url': 'http://dlvr.it/SPVc3m', 'display_url': 'dlvr.it/SPVc3m', 'images': [{'url': 'https://pbs.twimg.com/news_img/1600963220268896266/UCyMCryB?format=jpg&name=orig', 'width': 1440, 'height': 810}, {'url': 'https://pbs.twimg.com/news_img/1600963220268896266/UCyMCryB?format=jpg&name=150x150', 'width': 150, 'height': 150}], 'status': 200, 'title': 'Artificial intelligence challenges what it means to be creative', 'description': 'Computer programs can mimic famous artworks, but struggle with originality and lack self-awareness.', 'unwound_url': 'https://www.sciencenews.org/article/artificial-intelligence-ai-creativity-art-computer-program?utm_source=dlvr.it&utm_medium=twitter'}, {'start': 240, 'end': 263, 'url': 'https://t.co/9cNPF8Ptyb', 'expanded_url': 'https://twitter.com/SuriyaSubraman/status/1520189463766265857/photo/1', 'display_url': 'pic.twitter.com/9cNPF8Ptyb', 'media_key': '3_1520189460939386880'}], 'annotations': [{'start': 66, 'end': 86, 'probability': 0.8423, 'type': 'Other', 'normalized_text': 'Science News Magazine'}, {'start': 162, 'end': 174, 'probability': 0.7132, 'type': 'Other', 'normalized_text': 'News Magazine'}, {'start': 201, 'end': 202, 'probability': 0.5984, 'type': 'Organization', 'normalized_text': 'AI'}], 'hashtags': [{'start': 200, 'end': 203, 'tag': 'AI'}, {'start': 204, 'end': 227, 'tag': 'artificialintelligence'}, {'start': 228, 'end': 239, 'tag': 'Finperform'}]}   \n",
              "\n",
              "                                                                                                                                                                                                                                           text_clean  \\\n",
              "0                                                                                                                                                                               10 reasons why you should learn artificial intelligence  deeplearning   \n",
              "1  gods and goddesses gods and goddesses can range from artificial intelligence to earthly nature deities that all ultimately fall under the umbrella of higher dimensional   godsgoddesses naturedeities supernatural paranormal fantasy scifi asmsg   \n",
              "2                                              theres no shortage of data on customers these days companies are collecting reams of it to get the best insights on their customersai in customeranalytics   ai automation techupdate it tech industry   \n",
              "3                                                                                                                                             economics homework help hsm artificial intelligence  internet of things  its impact on delgro worksheet   \n",
              "4                                   artificial intelligence challenges what it means to be creative  science news magazine artificial intelligence challenges what it means to be creativescience news magazine  ai artificialintelligence finperform   \n",
              "\n",
              "   sentiment  \n",
              "0   3.803131  \n",
              "1   3.426736  \n",
              "2   4.272908  \n",
              "3   3.693248  \n",
              "4   3.441661  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8043eba9-c81c-4bc8-aaa9-4b18d7b068a0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at</th>\n",
              "      <th>screen_name</th>\n",
              "      <th>text</th>\n",
              "      <th>lang</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>like_count</th>\n",
              "      <th>quote_count</th>\n",
              "      <th>impression_count</th>\n",
              "      <th>id</th>\n",
              "      <th>author_id</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>in_reply_to_user_id</th>\n",
              "      <th>geo</th>\n",
              "      <th>entities</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-04-29T23:59:09.000Z</td>\n",
              "      <td>None</td>\n",
              "      <td>10 Reasons Why you Should Learn Artificial Intelligence https://t.co/nqYx4aS058 #deeplearning</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1520190984910479368</td>\n",
              "      <td>1013232376758128643</td>\n",
              "      <td>1520190984910479368</td>\n",
              "      <td>nan</td>\n",
              "      <td>None</td>\n",
              "      <td>{'urls': [{'start': 56, 'end': 79, 'url': 'https://t.co/nqYx4aS058', 'expanded_url': 'https://hackernoon.com/10-reasons-why-you-should-learn-artificial-intelligence-5v6q30vo', 'display_url': 'hackernoon.com/10-reasons-why…', 'status': 200, 'unwound_url': 'https://hackernoon.com/10-reasons-why-you-should-learn-artificial-intelligence-5v6q30vo'}], 'hashtags': [{'start': 80, 'end': 93, 'tag': 'deeplearning'}]}</td>\n",
              "      <td>10 reasons why you should learn artificial intelligence  deeplearning</td>\n",
              "      <td>3.803131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-04-29T23:56:12.000Z</td>\n",
              "      <td>None</td>\n",
              "      <td>Gods And Goddesses Gods, and Goddesses can range from artificial intelligence to Earthly nature deities that all ultimately fall under the umbrella of higher dimensional ... https://t.co/ODSA8XFEND #GodsGoddesses #NatureDeities #Supernatural #Paranormal #Fantasy #scifi #ASMSG</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1520190245341261825</td>\n",
              "      <td>388385054</td>\n",
              "      <td>1520190245341261825</td>\n",
              "      <td>nan</td>\n",
              "      <td>None</td>\n",
              "      <td>{'urls': [{'start': 174, 'end': 197, 'url': 'https://t.co/ODSA8XFEND', 'expanded_url': 'https://mysticinvestigations.com/gods/', 'display_url': 'mysticinvestigations.com/gods/', 'status': 200, 'unwound_url': 'https://mysticinvestigations.com/gods/'}], 'annotations': [{'start': 271, 'end': 275, 'probability': 0.5446, 'type': 'Other', 'normalized_text': 'ASMSG'}], 'hashtags': [{'start': 198, 'end': 212, 'tag': 'GodsGoddesses'}, {'start': 213, 'end': 227, 'tag': 'NatureDeities'}, {'start': 228, 'end': 241, 'tag': 'Supernatural'}, {'start': 242, 'end': 253, 'tag': 'Paranormal'}, {'start': 254, 'end': 262, 'tag': 'Fantasy'}, {'start': 263, 'end': 269, 'tag': 'scifi'}, {'start': 270, 'end': 276, 'tag': 'ASMSG'}]}</td>\n",
              "      <td>gods and goddesses gods and goddesses can range from artificial intelligence to earthly nature deities that all ultimately fall under the umbrella of higher dimensional   godsgoddesses naturedeities supernatural paranormal fantasy scifi asmsg</td>\n",
              "      <td>3.426736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-04-29T23:55:05.000Z</td>\n",
              "      <td>None</td>\n",
              "      <td>There’s no shortage of #data on customers these days. Companies are collecting reams of it to get the best insights on their customers.\\n\\nAI in #CustomerAnalytics - https://t.co/RmRMDPolFL \\n\\n#AI #Automation #TechUpdate #IT #tech #industry https://t.co/7QqKb3rS7Z</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1520189963614277633</td>\n",
              "      <td>1334030877018316801</td>\n",
              "      <td>1520189963614277633</td>\n",
              "      <td>nan</td>\n",
              "      <td>None</td>\n",
              "      <td>{'urls': [{'start': 164, 'end': 187, 'url': 'https://t.co/RmRMDPolFL', 'expanded_url': 'https://bit.ly/3vJ91SI', 'display_url': 'bit.ly/3vJ91SI', 'status': 200, 'unwound_url': 'https://www.itpro.co.uk/technology/artificial-intelligence-ai/361259/ai-in-customer-analytics'}, {'start': 238, 'end': 261, 'url': 'https://t.co/7QqKb3rS7Z', 'expanded_url': 'https://twitter.com/IgnitivOfficial/status/1520189963614277633/photo/1', 'display_url': 'pic.twitter.com/7QqKb3rS7Z', 'media_key': '3_1520189961424838657'}], 'annotations': [{'start': 137, 'end': 138, 'probability': 0.8035, 'type': 'Organization', 'normalized_text': 'AI'}, {'start': 191, 'end': 192, 'probability': 0.6384, 'type': 'Organization', 'normalized_text': 'AI'}], 'hashtags': [{'start': 23, 'end': 28, 'tag': 'data'}, {'start': 143, 'end': 161, 'tag': 'CustomerAnalytics'}, {'start': 190, 'end': 193, 'tag': 'AI'}, {'start': 194, 'end': 205, 'tag': 'Automation'}, {'start': 206, 'end': 217, 'tag': 'TechUpdate'}, {'start': 218, 'end': 221, 'tag': 'IT'}, {'start': 222, 'end': 227, 'tag': 'tech'}, {'start': 228, 'end': 237, 'tag': 'industry'}]}</td>\n",
              "      <td>theres no shortage of data on customers these days companies are collecting reams of it to get the best insights on their customersai in customeranalytics   ai automation techupdate it tech industry</td>\n",
              "      <td>4.272908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-04-29T23:53:40.000Z</td>\n",
              "      <td>None</td>\n",
              "      <td>Economics Homework Help HSM Artificial Intelligence &amp;amp; Internet of Things &amp;amp; Its Impact on Delgro Worksheet\\n\\n  https://t.co/q658tYPOsh</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1520189606162956289</td>\n",
              "      <td>1353946694120067074</td>\n",
              "      <td>1520189606162956289</td>\n",
              "      <td>nan</td>\n",
              "      <td>None</td>\n",
              "      <td>{'urls': [{'start': 117, 'end': 140, 'url': 'https://t.co/q658tYPOsh', 'expanded_url': 'https://besthomeworkservices.com/economics-homework-help-7780/?utm_source=ReviveOldPost&amp;utm_medium=social&amp;utm_campaign=ReviveOldPost', 'display_url': 'besthomeworkservices.com/economics-home…', 'status': 403, 'unwound_url': 'https://besthomeworkservices.com/economics-homework-help-7780/?utm_source=ReviveOldPost&amp;utm_medium=social&amp;utm_campaign=ReviveOldPost'}], 'annotations': [{'start': 24, 'end': 26, 'probability': 0.4441, 'type': 'Other', 'normalized_text': 'HSM'}, {'start': 97, 'end': 112, 'probability': 0.3944, 'type': 'Other', 'normalized_text': 'Delgro Worksheet'}]}</td>\n",
              "      <td>economics homework help hsm artificial intelligence  internet of things  its impact on delgro worksheet</td>\n",
              "      <td>3.693248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-04-29T23:53:06.000Z</td>\n",
              "      <td>None</td>\n",
              "      <td>Artificial intelligence challenges what it means to be creative - Science News Magazine: Artificial intelligence challenges what it means to be creative  Science News Magazine https://t.co/rGKIaITETH #AI #artificialintelligence #Finperform https://t.co/9cNPF8Ptyb</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1520189463766265857</td>\n",
              "      <td>856240505826496513</td>\n",
              "      <td>1520189463766265857</td>\n",
              "      <td>nan</td>\n",
              "      <td>None</td>\n",
              "      <td>{'urls': [{'start': 176, 'end': 199, 'url': 'https://t.co/rGKIaITETH', 'expanded_url': 'http://dlvr.it/SPVc3m', 'display_url': 'dlvr.it/SPVc3m', 'images': [{'url': 'https://pbs.twimg.com/news_img/1600963220268896266/UCyMCryB?format=jpg&amp;name=orig', 'width': 1440, 'height': 810}, {'url': 'https://pbs.twimg.com/news_img/1600963220268896266/UCyMCryB?format=jpg&amp;name=150x150', 'width': 150, 'height': 150}], 'status': 200, 'title': 'Artificial intelligence challenges what it means to be creative', 'description': 'Computer programs can mimic famous artworks, but struggle with originality and lack self-awareness.', 'unwound_url': 'https://www.sciencenews.org/article/artificial-intelligence-ai-creativity-art-computer-program?utm_source=dlvr.it&amp;utm_medium=twitter'}, {'start': 240, 'end': 263, 'url': 'https://t.co/9cNPF8Ptyb', 'expanded_url': 'https://twitter.com/SuriyaSubraman/status/1520189463766265857/photo/1', 'display_url': 'pic.twitter.com/9cNPF8Ptyb', 'media_key': '3_1520189460939386880'}], 'annotations': [{'start': 66, 'end': 86, 'probability': 0.8423, 'type': 'Other', 'normalized_text': 'Science News Magazine'}, {'start': 162, 'end': 174, 'probability': 0.7132, 'type': 'Other', 'normalized_text': 'News Magazine'}, {'start': 201, 'end': 202, 'probability': 0.5984, 'type': 'Organization', 'normalized_text': 'AI'}], 'hashtags': [{'start': 200, 'end': 203, 'tag': 'AI'}, {'start': 204, 'end': 227, 'tag': 'artificialintelligence'}, {'start': 228, 'end': 239, 'tag': 'Finperform'}]}</td>\n",
              "      <td>artificial intelligence challenges what it means to be creative  science news magazine artificial intelligence challenges what it means to be creativescience news magazine  ai artificialintelligence finperform</td>\n",
              "      <td>3.441661</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8043eba9-c81c-4bc8-aaa9-4b18d7b068a0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8043eba9-c81c-4bc8-aaa9-4b18d7b068a0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8043eba9-c81c-4bc8-aaa9-4b18d7b068a0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_tweets.to_csv('tweets_sentiment.csv')\n",
        "\n",
        "#download to machine and re-upload to google drive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6zjZpoy9XJe",
        "outputId": "03fbd8cb-6d2a-4c0b-db27-a801236ed63b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_tweets = pd.read_csv('../drive/MyDrive/tweets_sentiment.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhlIgbn-Xrii",
        "outputId": "98460712-ab7a-4b0c-f160-3630eafb67af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<ipython-input-14-83fc1b238181>:1: DtypeWarning: Columns (0,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_tweets = pd.read_csv('../drive/MyDrive/tweets_sentiment.csv')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_tweets.tail()"
      ],
      "metadata": {
        "id": "SekWnvmVbPGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Filter the dataset to include only tweets related to \"artificial intelligence\" or \"ai\"\n",
        "ai_df_tweets = df_tweets[df_tweets['text'].str.contains('artificial intelligence|ai', case=False)]\n",
        "\n",
        "# Convert the \"created_at\" column to a datetime object and set timezone\n",
        "ai_df_tweets['created_at'] = pd.to_datetime(ai_df_tweets['created_at']).dt.tz_localize(None).dt.tz_localize('UTC')\n",
        "\n",
        "# Filter the dataset to include only tweets from April 2022 to April 2023\n",
        "start_date = pd.to_datetime('2022-04-29').tz_localize('UTC')\n",
        "end_date = pd.to_datetime('2023-04-21').tz_localize('UTC')\n",
        "ai_df_tweets = ai_df_tweets[(ai_df_tweets['created_at'] >= start_date) & (ai_df_tweets['created_at'] <= end_date)]\n",
        "\n",
        "# Calculate the average sentiment score for each week\n",
        "average_sentiment_scores = ai_df_tweets.groupby(pd.Grouper(key='created_at', freq='W')).mean()['sentiment'].tolist()\n",
        "\n",
        "# Create a list of x-axis labels to display the first week of each month\n",
        "x_labels = []\n",
        "for date in pd.date_range(start_date, end_date, freq='W'):\n",
        "    if date.day <= 7:\n",
        "        x_labels.append(date.strftime('%b %d, %Y'))\n",
        "    else:\n",
        "        x_labels.append('')\n",
        "\n",
        "# Create a line chart for the average sentiment scores\n",
        "plt.plot(average_sentiment_scores)\n",
        "\n",
        "# Add title and axis labels\n",
        "plt.title('Average sentiment related to \"artificial intelligence\" and \"ai\" over time')\n",
        "plt.xlabel('Week')\n",
        "plt.ylabel('Sentiment Score (out of 5)')\n",
        "\n",
        "# Set the Y-axis limits to reflect the sentiment score range\n",
        "plt.ylim([2.5, 4])\n",
        "\n",
        "# Set the x-axis tick labels to only display the first week of each month\n",
        "plt.xticks(range(len(x_labels)), x_labels, rotation=45)\n",
        "\n",
        "# Display the chart\n",
        "plt.show()\n",
        "\n",
        "num_tweets = len(ai_df_tweets)\n",
        "print(f\"There are {num_tweets} tweets in the ai_df_tweets dataframe.\")"
      ],
      "metadata": {
        "id": "s8nYDh7CDNzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#edited\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "# Filter the dataset to include only tweets related to \"artificial intelligence\" or \"ai\"\n",
        "ai_df_tweets = df_tweets[df_tweets['text'].str.contains('artificial intelligence|ai', case=False)]\n",
        "\n",
        "# Convert the \"created_at\" column to a datetime object and set timezone\n",
        "ai_df_tweets['created_at'] = pd.to_datetime(ai_df_tweets['created_at']).dt.tz_localize(None).dt.tz_localize('UTC')\n",
        "\n",
        "# Filter the dataset to include only tweets from April 2022 to April 2023\n",
        "start_date = pd.to_datetime('2022-04-29').tz_localize('UTC')\n",
        "end_date = pd.to_datetime('2023-04-21').tz_localize('UTC')\n",
        "ai_df_tweets = ai_df_tweets[(ai_df_tweets['created_at'] >= start_date) & (ai_df_tweets['created_at'] <= end_date)]\n",
        "\n",
        "# Calculate the average sentiment score and standard deviation for each month\n",
        "monthly_sentiment = ai_df_tweets.groupby(pd.Grouper(key='created_at', freq='M')).agg({'sentiment': ['mean', 'std']})\n",
        "\n",
        "# Create a bar chart for the average sentiment scores\n",
        "x_labels = monthly_sentiment.index.strftime('%b %Y')\n",
        "x_pos = np.arange(len(x_labels))\n",
        "plt.bar(x_pos, monthly_sentiment['sentiment']['mean'], yerr=monthly_sentiment['sentiment']['std'], align='center', alpha=0.5)\n",
        "\n",
        "# Add title and axis labels\n",
        "plt.title('Average sentiment related to \"artificial intelligence\" and \"ai\" per month')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Sentiment Score (out of 5)')\n",
        "\n",
        "# Set the Y-axis limits to reflect the sentiment score range\n",
        "plt.ylim([2, 4.5])\n",
        "\n",
        "# Set the x-axis tick labels\n",
        "plt.xticks(x_pos, x_labels, rotation=45)\n",
        "\n",
        "# Increase the size of the figure\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(12, 6)\n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rysQctoGDYIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# edited\n",
        "# MH code for what do people talk about when they talk about AI (Wordclouds)\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Define stopwords to be removed from the wordcloud\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.update([\"artificial\", \"intelligence\", \"ai\", \"https\", \"artificialintelligence\"])\n",
        "\n",
        "# Define function to create wordcloud for a specific month\n",
        "def create_wordcloud_for_month(month):\n",
        "    # Filter tweets to only include those from the given month\n",
        "    month_tweets = ai_df_tweets[pd.DatetimeIndex(ai_df_tweets['created_at']).month == month]\n",
        "\n",
        "    # Remove the t co link shortener from the tweets\n",
        "    month_tweets['text'] = month_tweets['text'].str.replace(r'http\\S+|www.\\S+', '', case=False)\n",
        "\n",
        "    # Join all tweets into a single string\n",
        "    text = \" \".join(tweet for tweet in month_tweets.text)\n",
        "\n",
        "    # Generate the wordcloud\n",
        "    wordcloud = WordCloud(stopwords=stopwords,max_font_size=150,\n",
        "                          max_words=100,\n",
        "                          background_color=\"black\",\n",
        "                          width=1000,\n",
        "                          height=600).generate(text)\n",
        "\n",
        "    # Visualize the wordcloud\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Wordcloud for {pd.Timestamp(2022, month, 1).strftime('%B')}\")\n",
        "    plt.show()\n",
        "\n",
        "# Call the function for each specified month\n",
        "create_wordcloud_for_month(5)  # May\n",
        "create_wordcloud_for_month(6)  # June\n",
        "create_wordcloud_for_month(7)  # July\n",
        "create_wordcloud_for_month(8)  # August\n",
        "create_wordcloud_for_month(11) # November\n",
        "create_wordcloud_for_month(12) # December\n",
        "create_wordcloud_for_month(1) # January\n",
        "create_wordcloud_for_month(2)  # February\n",
        "create_wordcloud_for_month(3)  # March"
      ],
      "metadata": {
        "id": "hjwk6HxgDdPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Define stopwords to be removed from the wordcloud\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.update([\"artificial\", \"intelligence\", \"ai\", \"https\", \"artificialintelligence\"])\n",
        "\n",
        "# Define function to create wordcloud for a specific month\n",
        "def create_wordcloud_for_month(month):\n",
        "    # Filter tweets to only include those from the given month\n",
        "    month_tweets = ai_df_tweets[pd.DatetimeIndex(ai_df_tweets['created_at']).month == month]\n",
        "\n",
        "    # Filter tweets to only include those from April 2023\n",
        "    month_tweets = month_tweets[pd.DatetimeIndex(month_tweets['created_at']).year == 2023]\n",
        "\n",
        "    # Remove the t co link shortener from the tweets\n",
        "    month_tweets['text'] = month_tweets['text'].str.replace(r'http\\S+|www.\\S+', '', case=False)\n",
        "\n",
        "    # Join all tweets into a single string\n",
        "    text = \" \".join(tweet for tweet in month_tweets.text)\n",
        "\n",
        "    # Generate the wordcloud\n",
        "    wordcloud = WordCloud(stopwords=stopwords,max_font_size=150,\n",
        "                          max_words=100,\n",
        "                          background_color=\"black\",\n",
        "                          width=1000,\n",
        "                          height=600).generate(text)\n",
        "\n",
        "    # Visualize the wordcloud\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Wordcloud for {pd.Timestamp(2023, month, 1).strftime('%B')}\")\n",
        "    plt.show()\n",
        "\n",
        "# Call the function for April 2023\n",
        "create_wordcloud_for_month(4)  # April"
      ],
      "metadata": {
        "id": "FJmbKYkUDo82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to get random Learning python sample of tweets for a given month and keyword\n",
        "def get_random_tweets_for_month(month, keyword, num_tweets=5):\n",
        "    # Filter tweets to only include those from the given month\n",
        "    month_tweets = ai_df_tweets[pd.DatetimeIndex(ai_df_tweets['created_at']).month.isin([month])]\n",
        "\n",
        "    # Filter tweets to only include those containing the given keyword\n",
        "    month_tweets = month_tweets[month_tweets['text'].str.contains(keyword, case=False)]\n",
        "\n",
        "    # Remove the t co link shortener from the tweets\n",
        "    month_tweets['text'] = month_tweets['text'].str.replace(r'http\\S+|www.\\S+', '', case=False)\n",
        "\n",
        "    # Get a random sample of tweets\n",
        "    random_tweets = month_tweets.sample(num_tweets)\n",
        "\n",
        "    # Return the sample of tweets as a list\n",
        "    return random_tweets['text'].tolist()\n",
        "\n",
        "# Call the function to get a random sample of 5 tweets containing the word \"Python\" from September 2022\n",
        "get_random_tweets_for_month(9, 'python')"
      ],
      "metadata": {
        "id": "iy8O8w-CDuLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to get random chatgpt for January\n",
        "def get_random_tweets_for_month(month, keyword, num_tweets=5):\n",
        "    # Filter tweets to only include those from the given month\n",
        "    month_tweets = ai_df_tweets[pd.DatetimeIndex(ai_df_tweets['created_at']).month.isin([month])]\n",
        "\n",
        "    # Filter tweets to only include those containing the given keyword\n",
        "    month_tweets = month_tweets[month_tweets['text'].str.contains(keyword, case=False)]\n",
        "\n",
        "    # Remove the t co link shortener from the tweets\n",
        "    month_tweets['text'] = month_tweets['text'].str.replace(r'http\\S+|www.\\S+', '', case=False)\n",
        "\n",
        "    # Get a random sample of tweets\n",
        "    random_tweets = month_tweets.sample(num_tweets)\n",
        "\n",
        "    # Return the sample of tweets as a list\n",
        "    return random_tweets['text'].tolist()\n",
        "\n",
        "# Call the function to get a random sample of 5 tweets containing the word \"Python\" from September 2022\n",
        "get_random_tweets_for_month(1, 'chatgpt')"
      ],
      "metadata": {
        "id": "vOslNd2gDyZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to get random art for December\n",
        "def get_random_tweets_for_month(month, keyword, num_tweets=5):\n",
        "    # Filter tweets to only include those from the given month\n",
        "    month_tweets = ai_df_tweets[pd.DatetimeIndex(ai_df_tweets['created_at']).month.isin([month])]\n",
        "\n",
        "    # Filter tweets to only include those containing the given keyword\n",
        "    month_tweets = month_tweets[month_tweets['text'].str.contains(keyword, case=False)]\n",
        "\n",
        "    # Remove the t co link shortener from the tweets\n",
        "    month_tweets['text'] = month_tweets['text'].str.replace(r'http\\S+|www.\\S+', '', case=False)\n",
        "\n",
        "    # Get a random sample of tweets\n",
        "    random_tweets = month_tweets.sample(num_tweets)\n",
        "\n",
        "    # Return the sample of tweets as a list\n",
        "    return random_tweets['text'].tolist()\n",
        "\n",
        "# Call the function to get a random sample of 5 tweets containing the word \"Python\" from September 2022\n",
        "get_random_tweets_for_month(12, 'artist')"
      ],
      "metadata": {
        "id": "8jPajTwND2PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to get random chatgpt for December\n",
        "def get_random_tweets_for_month(month, keyword, num_tweets=5):\n",
        "    # Filter tweets to only include those from the given month\n",
        "    month_tweets = ai_df_tweets[pd.DatetimeIndex(ai_df_tweets['created_at']).month.isin([month])]\n",
        "\n",
        "    # Filter tweets to only include those containing the given keyword\n",
        "    month_tweets = month_tweets[month_tweets['text'].str.contains(keyword, case=False)]\n",
        "\n",
        "    # Remove the t co link shortener from the tweets\n",
        "    month_tweets['text'] = month_tweets['text'].str.replace(r'http\\S+|www.\\S+', '', case=False)\n",
        "\n",
        "    # Get a random sample of tweets\n",
        "    random_tweets = month_tweets.sample(num_tweets)\n",
        "\n",
        "    # Return the sample of tweets as a list\n",
        "    return random_tweets['text'].tolist()\n",
        "\n",
        "# Call the function to get a random sample of 5 tweets containing the word \"Python\" from September 2022\n",
        "get_random_tweets_for_month(12, 'chatgpt')"
      ],
      "metadata": {
        "id": "jBjzrSC_D7aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to get random match for April\n",
        "def get_random_tweets_for_month(month, keyword, num_tweets=5):\n",
        "    # Filter tweets to only include those from the given month\n",
        "    month_tweets = ai_df_tweets[pd.DatetimeIndex(ai_df_tweets['created_at']).month.isin([month])]\n",
        "\n",
        "    # Filter tweets to only include those containing the given keyword\n",
        "    month_tweets = month_tweets[month_tweets['text'].str.contains(keyword, case=False)]\n",
        "\n",
        "    # Remove the t co link shortener from the tweets\n",
        "    month_tweets['text'] = month_tweets['text'].str.replace(r'http\\S+|www.\\S+', '', case=False)\n",
        "\n",
        "    # Get a random sample of tweets\n",
        "    random_tweets = month_tweets.sample(num_tweets)\n",
        "\n",
        "    # Return the sample of tweets as a list\n",
        "    return random_tweets['text'].tolist()\n",
        "\n",
        "# Call the function to get a random sample of 5 tweets containing the word \"Python\" from September 2022\n",
        "get_random_tweets_for_month(4, 'token')"
      ],
      "metadata": {
        "id": "Xj--CMBRD-3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "zFa9EBaZgW0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_en = DB.fetch(table_name='keyword_tweets',path=fname_db)\n",
        "df_it = DB.fetch(table_name='italian_tweets',path=fname_db)"
      ],
      "metadata": {
        "id": "rwbIol2ZgvWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0\n",
        "Sentiment = []\n",
        "Embedding = []\n",
        "for index,row in df_it.iterrows():  #iterate over rows of dataframe\n",
        "    c+=1\n",
        "    if c%1000==0:print(f\"Tweet {c}/{len(df_it)}\")  #print progres every 1000 rows\n",
        "\n",
        "    sentiment,embedding = sentiment_classifier(row.text,model,tokenizer)  #calculate sentiment and embedding of tweet\n",
        "    Sentiment.append(sentiment)  #append sentiment of tweet to Sentiment list\n",
        "    Embedding.append(embedding) #append embedding of tweet to Embedding list\n",
        "\n",
        "df_it['sentiment'] = Sentiment  #add sentiment column to dataframe of tweets\n",
        "df_it.head()"
      ],
      "metadata": {
        "id": "7w8IqvURgeCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_en['created_date'] = df_en.apply(lambda row: row['created_at'].split('T')[0], axis=1)\n",
        "df_it['created_date'] = df_it.apply(lambda row: row['created_at'].split('T')[0], axis=1)\n",
        "\n",
        "avg_sentiment_by_date_en = df_en.groupby(['created_date'])['sentiment'].mean()\n",
        "avg_sentiment_by_date_it = df_it.groupby(['created_date'])['sentiment'].mean()\n",
        "\n",
        "df_sentiments = pd.concat([avg_sentiment_by_date_en, avg_sentiment_by_date_it], axis=1)\n",
        "df_sentiments = df_sentiments.dropna()\n",
        "\n",
        "df_sentiments.columns = ['en_sentiment','it_sentiment']"
      ],
      "metadata": {
        "id": "Lpyr6bujgmSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.lineplot(data=df_sentiments[['en_sentiment','it_sentiment']])\n",
        "plt.xticks(rotation=90, fontsize = 'xx-small')\n",
        "plt.title('Tweet Sentiment about ChatGPT')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6rRDJ8KKhNXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    stop_words = set(stopwords.words('italian'))\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    cleaned_tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    cleaned_text = ' '.join(cleaned_tokens)\n",
        "    return cleaned_text\n",
        "\n",
        "# Clean the text of the tweets\n",
        "df_it['cleaned_text'] = df_it['text'].apply(clean_text)\n",
        "\n",
        "# Create a list of the months for which we want to create word clouds\n",
        "dates = ['2022-12-20', '2023-04-04']\n",
        "\n",
        "# Loop through the months and create a word cloud for each month\n",
        "for date in dates:\n",
        "    # Filter the dataset to include only tweets from the specified month\n",
        "    df_tweets = df_it[df_it['created_date'].str.contains(date)]\n",
        "\n",
        "    # Concatenate all the cleaned text from the tweets into a single string\n",
        "    text = ' '.join(df_tweets['cleaned_text'].tolist())\n",
        "\n",
        "    # Create a word cloud for the month\n",
        "    wordcloud = WordCloud(width=800, height=800, background_color='white').generate(text)\n",
        "\n",
        "    # Display the word cloud\n",
        "    plt.figure(figsize=(8, 8), facecolor=None)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.title('Word Cloud for ' + date)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GfHCO6LDhN6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_it[df_it['created_date'] == '2023-04-04'].sort_values(by='sentiment',ascending=True).head()"
      ],
      "metadata": {
        "id": "hoGanEzohWbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zxJw06R2FuO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "vglf1D66iALn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scripts.TextAnalysis as ta\n",
        "from scripts.api import *\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import codecs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXI-L4dTxG7T",
        "outputId": "57d6a3bf-8794-4acc-d33a-13e297d2e79c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import codecs  #this let's us display tweets properly (emojis, etc.)\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "metadata": {
        "id": "8_g17QY6i9ay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c00787e9-fc44-4337-cb20-1bf4cf49991f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"
      ],
      "metadata": {
        "id": "kjxMzY-NiM5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname_db = '/content/drive/MyDrive/SMAFP/AItweets_v2'\n",
        "df = DB.fetch(table_name='keyword_tweets',path=fname_db)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "7jngqq50iPJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_clean'] = df.text.apply(ta.clean_tweet)\n",
        "df = df[df.text_clean.str.len() >0]\n",
        "nclean = len(df)\n",
        "\n",
        "df.sample(n=5)"
      ],
      "metadata": {
        "id": "pKZZaoixiRUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[~df['text_clean'].str.contains('health consultations we are revolutionizing the way people access medical')]\n",
        "df = df[~df['text_clean'].str.contains('ai revolution download to participate in the ai2earn economy')]\n",
        "df = df[~df['text_clean'].str.contains('total maximum supply only 10 units a great artificial intelligence project dont miss')]\n",
        "df = df[~df['text_clean'].str.contains('blockchain based virtual world that allows users to create build buy and sell')]\n",
        "df = df[~df['text_clean'].str.contains('neurobayes fairlaunch starts at')]\n",
        "df = df[~df['text_clean'].str.contains('live happily togesssa')]"
      ],
      "metadata": {
        "id": "qyW2TZ3xiW3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_df = df[df['text'].str.contains('ChatGPT|chatgpt|GPT|gpt')]\n",
        "len(gpt_df)"
      ],
      "metadata": {
        "id": "9IbyoE1giXqX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19aad63-c017-4a5b-8b70-bf6e268f4d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11365"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dalle_df = df[df['text'].str.contains('dalle|dall-e|Dalle|Dall-E|DallE|Dall-e|dall-E|dallE')]\n",
        "len(dalle_df)"
      ],
      "metadata": {
        "id": "Npu1MAE2ibrf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "911c5237-dd98-4abb-a336-b2b494bd23b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2152"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mid_df = df[df['text'].str.contains('midjourney|Midjourney|midJourney|MidJourney')]\n",
        "len(mid_df)"
      ],
      "metadata": {
        "id": "Hh9FOfnUidgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f025aced-fcbd-4647-d129-94b1625380c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5636"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_classifier(text,model,tokenizer):\n",
        "    inputs = tokenizer.encode_plus(text, return_tensors='pt', add_special_tokens=True)\n",
        "\n",
        "    token_type_ids = inputs['token_type_ids']\n",
        "    input_ids = inputs['input_ids']\n",
        "\n",
        "    output = model(input_ids, token_type_ids=token_type_ids,return_dict=True,output_hidden_states=True)\n",
        "    logits = np.array(output.logits.tolist()[0])\n",
        "    prob = np.exp(logits)/np.sum(np.exp(logits))\n",
        "    sentiment = np.sum([(x+1)*prob[x] for x in range(len(prob))])  #use this line if you want the mean score\n",
        "    embedding = output.hidden_states[12].detach().numpy().squeeze()[0]\n",
        "\n",
        "    return sentiment,embedding\n",
        "\n",
        "c = 0\n",
        "Sentiment = []\n",
        "Embedding = []\n",
        "for index,row in gpt_df.iterrows():\n",
        "    c+=1\n",
        "    if c%1000==0:print(f\"Tweet {c}/{len(gpt_df)}\")\n",
        "\n",
        "    sentiment,embedding = sentiment_classifier(row.text,model,tokenizer)\n",
        "    Sentiment.append(sentiment)\n",
        "    Embedding.append(embedding)\n",
        "\n",
        "gpt_df['sentiment'] = Sentiment\n",
        "gpt_df.head()"
      ],
      "metadata": {
        "id": "2DIS3l3Din4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_sentiment = gpt_df['sentiment'].mean()\n",
        "print(f\"The mean opinion is {mean_sentiment}\")"
      ],
      "metadata": {
        "id": "b4ITZIkIirQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c87e3438-a563-4661-b2e8-7428841e5c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean opinion is 3.1319764752572326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_classifier(text,model,tokenizer):\n",
        "    inputs = tokenizer.encode_plus(text, return_tensors='pt', add_special_tokens=True)\n",
        "\n",
        "    token_type_ids = inputs['token_type_ids']\n",
        "    input_ids = inputs['input_ids']\n",
        "\n",
        "    output = model(input_ids, token_type_ids=token_type_ids,return_dict=True,output_hidden_states=True)\n",
        "    logits = np.array(output.logits.tolist()[0])\n",
        "    prob = np.exp(logits)/np.sum(np.exp(logits))\n",
        "    sentiment = np.sum([(x+1)*prob[x] for x in range(len(prob))])  #use this line if you want the mean score\n",
        "    embedding = output.hidden_states[12].detach().numpy().squeeze()[0]\n",
        "\n",
        "    return sentiment,embedding\n",
        "\n",
        "c = 0\n",
        "Sentiment = []\n",
        "Embedding = []\n",
        "for index,row in dalle_df.iterrows():\n",
        "    c+=1\n",
        "    if c%1000==0:print(f\"Tweet {c}/{len(dalle_df)}\")\n",
        "\n",
        "    sentiment,embedding = sentiment_classifier(row.text,model,tokenizer)\n",
        "    Sentiment.append(sentiment)\n",
        "    Embedding.append(embedding)\n",
        "\n",
        "dalle_df['sentiment'] = Sentiment\n",
        "dalle_df.head()"
      ],
      "metadata": {
        "id": "h8N_3w8lit-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_sentiment = dalle_df['sentiment'].mean()\n",
        "print(f\"The mean opinion is {mean_sentiment}\")"
      ],
      "metadata": {
        "id": "jw8ZehiuixBE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad55796b-124f-48fe-b53a-1957c8f3b9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean opinion is 3.1411879942290373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_classifier(text,model,tokenizer):\n",
        "    inputs = tokenizer.encode_plus(text, return_tensors='pt', add_special_tokens=True)\n",
        "\n",
        "    token_type_ids = inputs['token_type_ids']\n",
        "    input_ids = inputs['input_ids']\n",
        "\n",
        "    output = model(input_ids, token_type_ids=token_type_ids,return_dict=True,output_hidden_states=True)\n",
        "    logits = np.array(output.logits.tolist()[0])\n",
        "    prob = np.exp(logits)/np.sum(np.exp(logits))\n",
        "    sentiment = np.sum([(x+1)*prob[x] for x in range(len(prob))])  #use this line if you want the mean score\n",
        "    embedding = output.hidden_states[12].detach().numpy().squeeze()[0]\n",
        "\n",
        "    return sentiment,embedding\n",
        "\n",
        "c = 0\n",
        "Sentiment = []\n",
        "Embedding = []\n",
        "for index,row in mid_df.iterrows():\n",
        "    c+=1\n",
        "    if c%1000==0:print(f\"Tweet {c}/{len(mid_df)}\")\n",
        "\n",
        "    sentiment,embedding = sentiment_classifier(row.text,model,tokenizer)\n",
        "    Sentiment.append(sentiment)\n",
        "    Embedding.append(embedding)\n",
        "\n",
        "mid_df['sentiment'] = Sentiment\n",
        "mid_df.head()"
      ],
      "metadata": {
        "id": "fM6YFRrtizIP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "03751451-7676-4954-aa49-991ebae8dd62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet 1000/5636\n",
            "Tweet 2000/5636\n",
            "Tweet 3000/5636\n",
            "Tweet 4000/5636\n",
            "Tweet 5000/5636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-926c7ab9dfd5>:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  mid_df['sentiment'] = Sentiment\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   created_at screen_name  \\\n",
              "321  2023-04-25T20:33:16.000Z        None   \n",
              "463  2023-04-25T20:23:52.000Z        None   \n",
              "664  2023-04-25T20:11:06.000Z        None   \n",
              "702  2023-04-25T20:08:48.000Z        None   \n",
              "721  2023-04-25T20:07:33.000Z        None   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                  text  \\\n",
              "321                                                                                                                                                                                              Check out my latest article: Designing T-Shirts with #ChatGPT and #Midjourney \\nhttps://t.co/JYU3shbrYT via @LinkedIn   \n",
              "463           💥 Captains, the moment has come! 🦸🏻🦹🏼 \\n\\n🦋 \"The Metamorphosis\" is LIVE!\\n\\nExplore your heroes' stunning transformations and dive into new adventures 🌌 \\n\\nJoin us now to check the beta👷🏽at: https://t.co/7p15zt8o4y \\n\\n#AIU #NFT #Metamorphosis #AIArt #Midjourney #ChatGPT https://t.co/f3wscifSeM   \n",
              "664  #MidJourney #OpenAi #AiArt #Art #OpenAi #StableDiffusion2 #DallE #ChatGPT #AiArtworks\\n\\n#imagine \\n🌺  DETAILED PROMPT  👇🏼| CREATE YOUR OWN \\n\\nsamsara wallpaper, in the style of lit kid, time-lapse photography, moody figurative, forced perspective, mysterious backdrops, patience… https://t.co/vuFo11Sicg   \n",
              "702                                                                                                                   Almost perfect. Sun is going down like in the Elton John song. \\n\\nI would understand chatGPT having these songs in its lyrics. Midjourney is supposed to be just images https://t.co/W4zRqjEENc   \n",
              "721                                                                                                                                                      @TomBilyeu Check this one out from @mreflow \\n\\nGenerated with:\\nChatGPT, ElevenLabs, Mubert, Gen2, MidJourney, LeiaPix, &amp; Genmo\\nhttps://t.co/5LtaJujnOd   \n",
              "\n",
              "    lang  retweet_count  reply_count  like_count  quote_count  \\\n",
              "321   en              0            0           1            0   \n",
              "463   en              4            4           7            0   \n",
              "664   en              1            0           0            0   \n",
              "702   en              0            1           0            0   \n",
              "721   en              1            0           1            0   \n",
              "\n",
              "     impression_count                   id            author_id  \\\n",
              "321                16  1650961193065119745              6875652   \n",
              "463                88  1650958826449825804  1518932514244599809   \n",
              "664                 6  1650955615823360011  1563511792915316738   \n",
              "702                51  1650955037856653312            476490904   \n",
              "721                80  1650954723178971136             19281485   \n",
              "\n",
              "         conversation_id in_reply_to_user_id  geo  \\\n",
              "321  1650961193065119745                 nan  nan   \n",
              "463  1650958826449825804                 nan  nan   \n",
              "664  1650955615823360011                 nan  nan   \n",
              "702  1650953744723374083           476490904  nan   \n",
              "721  1650944450116608000          1087646485  nan   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            entities  \\\n",
              "321                                                                                                                                                                              {'urls': [{'start': 79, 'end': 102, 'url': 'https://t.co/JYU3shbrYT', 'expanded_url': 'https://www.linkedin.com/pulse/designing-t-shirts-chatgpt-midjourney-marco-van-hylckama-vlieg', 'display_url': 'linkedin.com/pulse/designin…', 'images': [{'url': 'https://pbs.twimg.com/news_img/1650961194671538176/4OjLiuqn?format=png&name=orig', 'width': 1400, 'height': 800}, {'url': 'https://pbs.twimg.com/news_img/1650961194671538176/4OjLiuqn?format=png&name=150x150', 'width': 150, 'height': 150}], 'status': 200, 'title': 'Designing T-Shirts with ChatGPT and Midjourney', 'description': \"I recently performed an experiment that's way too cool not to at least write a little article about. I am an avid T-shirt designer as a fun side gig besides my day job as Sr.\", 'unwound_url': 'https://www.linkedin.com/pulse/designing-t-shirts-chatgpt-midjourney-marco-van-hylckama-vlieg'}], 'hashtags': [{'start': 53, 'end': 61, 'tag': 'ChatGPT'}, {'start': 66, 'end': 77, 'tag': 'Midjourney'}], 'mentions': [{'start': 107, 'end': 116, 'username': 'LinkedIn', 'id': '13058772'}]}   \n",
              "463  {'hashtags': [{'start': 212, 'end': 216, 'tag': 'AIU'}, {'start': 217, 'end': 221, 'tag': 'NFT'}, {'start': 222, 'end': 236, 'tag': 'Metamorphosis'}, {'start': 237, 'end': 243, 'tag': 'AIArt'}, {'start': 244, 'end': 255, 'tag': 'Midjourney'}, {'start': 256, 'end': 264, 'tag': 'ChatGPT'}], 'urls': [{'start': 186, 'end': 209, 'url': 'https://t.co/7p15zt8o4y', 'expanded_url': 'https://warp-drive-eight.vercel.app/', 'display_url': 'warp-drive-eight.vercel.app', 'status': 200, 'title': 'ESTABLISHED CONNECTION WITH:', 'unwound_url': 'https://warp-drive-eight.vercel.app/'}, {'start': 265, 'end': 288, 'url': 'https://t.co/f3wscifSeM', 'expanded_url': 'https://twitter.com/AI_UniverseNFT/status/1650958826449825804/photo/1', 'display_url': 'pic.twitter.com/f3wscifSeM', 'media_key': '3_1650958819768299553'}], 'annotations': [{'start': 43, 'end': 59, 'probability': 0.948, 'type': 'Other', 'normalized_text': 'The Metamorphosis'}, {'start': 213, 'end': 215, 'probability': 0.3448, 'type': 'Organization', 'normalized_text': 'AIU'}, {'start': 223, 'end': 235, 'probability': 0.5315, 'type': 'Other', 'normalized_text': 'Metamorphosis'}, {'start': 238, 'end': 242, 'probability': 0.4861, 'type': 'Other', 'normalized_text': 'AIArt'}]}   \n",
              "664                                                                                                                                                                                                                                                                                                                                                                                                                                     {'annotations': [{'start': 59, 'end': 63, 'probability': 0.4554, 'type': 'Other', 'normalized_text': 'DallE'}], 'hashtags': [{'start': 0, 'end': 11, 'tag': 'MidJourney'}, {'start': 12, 'end': 19, 'tag': 'OpenAi'}, {'start': 20, 'end': 26, 'tag': 'AiArt'}, {'start': 27, 'end': 31, 'tag': 'Art'}, {'start': 32, 'end': 39, 'tag': 'OpenAi'}, {'start': 40, 'end': 57, 'tag': 'StableDiffusion2'}, {'start': 58, 'end': 64, 'tag': 'DallE'}, {'start': 65, 'end': 73, 'tag': 'ChatGPT'}, {'start': 74, 'end': 85, 'tag': 'AiArtworks'}, {'start': 87, 'end': 95, 'tag': 'imagine'}], 'urls': [{'start': 277, 'end': 300, 'url': 'https://t.co/vuFo11Sicg', 'expanded_url': 'https://twitter.com/MidJourneyAI_/status/1650955615823360011/photo/1', 'display_url': 'pic.twitter.com/vuFo11Sicg', 'media_key': '3_1650955613852037138'}]}   \n",
              "702                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {'annotations': [{'start': 16, 'end': 18, 'probability': 0.449, 'type': 'Organization', 'normalized_text': 'Sun'}, {'start': 46, 'end': 55, 'probability': 0.9214, 'type': 'Person', 'normalized_text': 'Elton John'}, {'start': 84, 'end': 90, 'probability': 0.7246, 'type': 'Other', 'normalized_text': 'chatGPT'}, {'start': 126, 'end': 135, 'probability': 0.6149, 'type': 'Other', 'normalized_text': 'Midjourney'}], 'urls': [{'start': 167, 'end': 190, 'url': 'https://t.co/W4zRqjEENc', 'expanded_url': 'https://twitter.com/quipsy/status/1650955037856653312/photo/1', 'display_url': 'pic.twitter.com/W4zRqjEENc', 'media_key': '3_1650955024564908066'}]}   \n",
              "721                                                                                                                                            {'mentions': [{'start': 0, 'end': 10, 'username': 'TomBilyeu', 'id': '1087646485'}, {'start': 35, 'end': 43, 'username': 'mreflow', 'id': '1544387652811493377'}], 'annotations': [{'start': 62, 'end': 68, 'probability': 0.9169, 'type': 'Other', 'normalized_text': 'ChatGPT'}, {'start': 71, 'end': 80, 'probability': 0.8462, 'type': 'Other', 'normalized_text': 'ElevenLabs'}, {'start': 83, 'end': 88, 'probability': 0.5968, 'type': 'Other', 'normalized_text': 'Mubert'}, {'start': 91, 'end': 94, 'probability': 0.8015, 'type': 'Other', 'normalized_text': 'Gen2'}, {'start': 97, 'end': 106, 'probability': 0.8167, 'type': 'Other', 'normalized_text': 'MidJourney'}, {'start': 109, 'end': 115, 'probability': 0.8468, 'type': 'Other', 'normalized_text': 'LeiaPix'}, {'start': 124, 'end': 128, 'probability': 0.6171, 'type': 'Other', 'normalized_text': 'Genmo'}], 'urls': [{'start': 130, 'end': 153, 'url': 'https://t.co/5LtaJujnOd', 'expanded_url': 'https://twitter.com/mreflow/status/1650669361311674368/video/1', 'display_url': 'pic.twitter.com/5LtaJujnOd', 'media_key': '7_1650668361532198912'}]}   \n",
              "\n",
              "                                                                                                                                                                                                                                                    text_clean  \\\n",
              "321                                                                                                                                                                    check out my latest article designing tshirts with chatgpt and midjourney  via linkedin   \n",
              "463                                         captains the moment has come   the metamorphosis is liveexplore your heroes stunning transformations and dive into new adventures  join us now to check the betaat  aiu nft metamorphosis aiart midjourney chatgpt   \n",
              "664  midjourney openai aiart art openai stablediffusion2 dalle chatgpt aiartworksimagine   detailed prompt   create your own samsara wallpaper in the style of lit kid timelapse photography moody figurative forced perspective mysterious backdrops patience   \n",
              "702                                                                                          almost perfect sun is going down like in the elton john song i would understand chatgpt having these songs in its lyrics midjourney is supposed to be just images   \n",
              "721                                                                                                                                           tombilyeu check this one out from mreflow generated withchatgpt elevenlabs mubert gen2 midjourney leiapix  genmo   \n",
              "\n",
              "     sentiment  \n",
              "321   3.459680  \n",
              "463   4.439003  \n",
              "664   3.949779  \n",
              "702   3.822272  \n",
              "721   2.878978  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-db84998a-dcff-43f7-9be6-26f94912e9af\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at</th>\n",
              "      <th>screen_name</th>\n",
              "      <th>text</th>\n",
              "      <th>lang</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>like_count</th>\n",
              "      <th>quote_count</th>\n",
              "      <th>impression_count</th>\n",
              "      <th>id</th>\n",
              "      <th>author_id</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>in_reply_to_user_id</th>\n",
              "      <th>geo</th>\n",
              "      <th>entities</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>2023-04-25T20:33:16.000Z</td>\n",
              "      <td>None</td>\n",
              "      <td>Check out my latest article: Designing T-Shirts with #ChatGPT and #Midjourney \\nhttps://t.co/JYU3shbrYT via @LinkedIn</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>1650961193065119745</td>\n",
              "      <td>6875652</td>\n",
              "      <td>1650961193065119745</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>{'urls': [{'start': 79, 'end': 102, 'url': 'https://t.co/JYU3shbrYT', 'expanded_url': 'https://www.linkedin.com/pulse/designing-t-shirts-chatgpt-midjourney-marco-van-hylckama-vlieg', 'display_url': 'linkedin.com/pulse/designin…', 'images': [{'url': 'https://pbs.twimg.com/news_img/1650961194671538176/4OjLiuqn?format=png&amp;name=orig', 'width': 1400, 'height': 800}, {'url': 'https://pbs.twimg.com/news_img/1650961194671538176/4OjLiuqn?format=png&amp;name=150x150', 'width': 150, 'height': 150}], 'status': 200, 'title': 'Designing T-Shirts with ChatGPT and Midjourney', 'description': \"I recently performed an experiment that's way too cool not to at least write a little article about. I am an avid T-shirt designer as a fun side gig besides my day job as Sr.\", 'unwound_url': 'https://www.linkedin.com/pulse/designing-t-shirts-chatgpt-midjourney-marco-van-hylckama-vlieg'}], 'hashtags': [{'start': 53, 'end': 61, 'tag': 'ChatGPT'}, {'start': 66, 'end': 77, 'tag': 'Midjourney'}], 'mentions': [{'start': 107, 'end': 116, 'username': 'LinkedIn', 'id': '13058772'}]}</td>\n",
              "      <td>check out my latest article designing tshirts with chatgpt and midjourney  via linkedin</td>\n",
              "      <td>3.459680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>2023-04-25T20:23:52.000Z</td>\n",
              "      <td>None</td>\n",
              "      <td>💥 Captains, the moment has come! 🦸🏻🦹🏼 \\n\\n🦋 \"The Metamorphosis\" is LIVE!\\n\\nExplore your heroes' stunning transformations and dive into new adventures 🌌 \\n\\nJoin us now to check the beta👷🏽at: https://t.co/7p15zt8o4y \\n\\n#AIU #NFT #Metamorphosis #AIArt #Midjourney #ChatGPT https://t.co/f3wscifSeM</td>\n",
              "      <td>en</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>88</td>\n",
              "      <td>1650958826449825804</td>\n",
              "      <td>1518932514244599809</td>\n",
              "      <td>1650958826449825804</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>{'hashtags': [{'start': 212, 'end': 216, 'tag': 'AIU'}, {'start': 217, 'end': 221, 'tag': 'NFT'}, {'start': 222, 'end': 236, 'tag': 'Metamorphosis'}, {'start': 237, 'end': 243, 'tag': 'AIArt'}, {'start': 244, 'end': 255, 'tag': 'Midjourney'}, {'start': 256, 'end': 264, 'tag': 'ChatGPT'}], 'urls': [{'start': 186, 'end': 209, 'url': 'https://t.co/7p15zt8o4y', 'expanded_url': 'https://warp-drive-eight.vercel.app/', 'display_url': 'warp-drive-eight.vercel.app', 'status': 200, 'title': 'ESTABLISHED CONNECTION WITH:', 'unwound_url': 'https://warp-drive-eight.vercel.app/'}, {'start': 265, 'end': 288, 'url': 'https://t.co/f3wscifSeM', 'expanded_url': 'https://twitter.com/AI_UniverseNFT/status/1650958826449825804/photo/1', 'display_url': 'pic.twitter.com/f3wscifSeM', 'media_key': '3_1650958819768299553'}], 'annotations': [{'start': 43, 'end': 59, 'probability': 0.948, 'type': 'Other', 'normalized_text': 'The Metamorphosis'}, {'start': 213, 'end': 215, 'probability': 0.3448, 'type': 'Organization', 'normalized_text': 'AIU'}, {'start': 223, 'end': 235, 'probability': 0.5315, 'type': 'Other', 'normalized_text': 'Metamorphosis'}, {'start': 238, 'end': 242, 'probability': 0.4861, 'type': 'Other', 'normalized_text': 'AIArt'}]}</td>\n",
              "      <td>captains the moment has come   the metamorphosis is liveexplore your heroes stunning transformations and dive into new adventures  join us now to check the betaat  aiu nft metamorphosis aiart midjourney chatgpt</td>\n",
              "      <td>4.439003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>664</th>\n",
              "      <td>2023-04-25T20:11:06.000Z</td>\n",
              "      <td>None</td>\n",
              "      <td>#MidJourney #OpenAi #AiArt #Art #OpenAi #StableDiffusion2 #DallE #ChatGPT #AiArtworks\\n\\n#imagine \\n🌺  DETAILED PROMPT  👇🏼| CREATE YOUR OWN \\n\\nsamsara wallpaper, in the style of lit kid, time-lapse photography, moody figurative, forced perspective, mysterious backdrops, patience… https://t.co/vuFo11Sicg</td>\n",
              "      <td>en</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1650955615823360011</td>\n",
              "      <td>1563511792915316738</td>\n",
              "      <td>1650955615823360011</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>{'annotations': [{'start': 59, 'end': 63, 'probability': 0.4554, 'type': 'Other', 'normalized_text': 'DallE'}], 'hashtags': [{'start': 0, 'end': 11, 'tag': 'MidJourney'}, {'start': 12, 'end': 19, 'tag': 'OpenAi'}, {'start': 20, 'end': 26, 'tag': 'AiArt'}, {'start': 27, 'end': 31, 'tag': 'Art'}, {'start': 32, 'end': 39, 'tag': 'OpenAi'}, {'start': 40, 'end': 57, 'tag': 'StableDiffusion2'}, {'start': 58, 'end': 64, 'tag': 'DallE'}, {'start': 65, 'end': 73, 'tag': 'ChatGPT'}, {'start': 74, 'end': 85, 'tag': 'AiArtworks'}, {'start': 87, 'end': 95, 'tag': 'imagine'}], 'urls': [{'start': 277, 'end': 300, 'url': 'https://t.co/vuFo11Sicg', 'expanded_url': 'https://twitter.com/MidJourneyAI_/status/1650955615823360011/photo/1', 'display_url': 'pic.twitter.com/vuFo11Sicg', 'media_key': '3_1650955613852037138'}]}</td>\n",
              "      <td>midjourney openai aiart art openai stablediffusion2 dalle chatgpt aiartworksimagine   detailed prompt   create your own samsara wallpaper in the style of lit kid timelapse photography moody figurative forced perspective mysterious backdrops patience</td>\n",
              "      <td>3.949779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>702</th>\n",
              "      <td>2023-04-25T20:08:48.000Z</td>\n",
              "      <td>None</td>\n",
              "      <td>Almost perfect. Sun is going down like in the Elton John song. \\n\\nI would understand chatGPT having these songs in its lyrics. Midjourney is supposed to be just images https://t.co/W4zRqjEENc</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>51</td>\n",
              "      <td>1650955037856653312</td>\n",
              "      <td>476490904</td>\n",
              "      <td>1650953744723374083</td>\n",
              "      <td>476490904</td>\n",
              "      <td>nan</td>\n",
              "      <td>{'annotations': [{'start': 16, 'end': 18, 'probability': 0.449, 'type': 'Organization', 'normalized_text': 'Sun'}, {'start': 46, 'end': 55, 'probability': 0.9214, 'type': 'Person', 'normalized_text': 'Elton John'}, {'start': 84, 'end': 90, 'probability': 0.7246, 'type': 'Other', 'normalized_text': 'chatGPT'}, {'start': 126, 'end': 135, 'probability': 0.6149, 'type': 'Other', 'normalized_text': 'Midjourney'}], 'urls': [{'start': 167, 'end': 190, 'url': 'https://t.co/W4zRqjEENc', 'expanded_url': 'https://twitter.com/quipsy/status/1650955037856653312/photo/1', 'display_url': 'pic.twitter.com/W4zRqjEENc', 'media_key': '3_1650955024564908066'}]}</td>\n",
              "      <td>almost perfect sun is going down like in the elton john song i would understand chatgpt having these songs in its lyrics midjourney is supposed to be just images</td>\n",
              "      <td>3.822272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>721</th>\n",
              "      <td>2023-04-25T20:07:33.000Z</td>\n",
              "      <td>None</td>\n",
              "      <td>@TomBilyeu Check this one out from @mreflow \\n\\nGenerated with:\\nChatGPT, ElevenLabs, Mubert, Gen2, MidJourney, LeiaPix, &amp;amp; Genmo\\nhttps://t.co/5LtaJujnOd</td>\n",
              "      <td>en</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>1650954723178971136</td>\n",
              "      <td>19281485</td>\n",
              "      <td>1650944450116608000</td>\n",
              "      <td>1087646485</td>\n",
              "      <td>nan</td>\n",
              "      <td>{'mentions': [{'start': 0, 'end': 10, 'username': 'TomBilyeu', 'id': '1087646485'}, {'start': 35, 'end': 43, 'username': 'mreflow', 'id': '1544387652811493377'}], 'annotations': [{'start': 62, 'end': 68, 'probability': 0.9169, 'type': 'Other', 'normalized_text': 'ChatGPT'}, {'start': 71, 'end': 80, 'probability': 0.8462, 'type': 'Other', 'normalized_text': 'ElevenLabs'}, {'start': 83, 'end': 88, 'probability': 0.5968, 'type': 'Other', 'normalized_text': 'Mubert'}, {'start': 91, 'end': 94, 'probability': 0.8015, 'type': 'Other', 'normalized_text': 'Gen2'}, {'start': 97, 'end': 106, 'probability': 0.8167, 'type': 'Other', 'normalized_text': 'MidJourney'}, {'start': 109, 'end': 115, 'probability': 0.8468, 'type': 'Other', 'normalized_text': 'LeiaPix'}, {'start': 124, 'end': 128, 'probability': 0.6171, 'type': 'Other', 'normalized_text': 'Genmo'}], 'urls': [{'start': 130, 'end': 153, 'url': 'https://t.co/5LtaJujnOd', 'expanded_url': 'https://twitter.com/mreflow/status/1650669361311674368/video/1', 'display_url': 'pic.twitter.com/5LtaJujnOd', 'media_key': '7_1650668361532198912'}]}</td>\n",
              "      <td>tombilyeu check this one out from mreflow generated withchatgpt elevenlabs mubert gen2 midjourney leiapix  genmo</td>\n",
              "      <td>2.878978</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db84998a-dcff-43f7-9be6-26f94912e9af')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-db84998a-dcff-43f7-9be6-26f94912e9af button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-db84998a-dcff-43f7-9be6-26f94912e9af');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_sentiment = mid_df['sentiment'].mean()\n",
        "print(f\"The mean opinion is {mean_sentiment}\")"
      ],
      "metadata": {
        "id": "NGzJMn3pi2BY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8aacf2b-18ba-41ec-f842-38fe66a035af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean opinion is 3.2872162117108004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "from statsmodels.stats.weightstats import ztest\n",
        "from statsmodels.stats.multitest import multipletests"
      ],
      "metadata": {
        "id": "plFUZ9Q5i4SP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d86f1ec5-b508-4bd4-9bf9-a482a6a9eee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X0 = gpt_df['sentiment']\n",
        "X1 = dalle_df['sentiment']\n",
        "\n",
        "alpha = 0.01\n",
        "tstat, pval = stats.ttest_ind(X0,X1, equal_var = False)\n",
        "print(f'T-Statistic: {tstat:.4f}\\nP-Value: {pval:.4f}')\n",
        "if pval < alpha:\n",
        "    print(\"Significant at a 1% level.\")\n",
        "else:\n",
        "    print(\"Not significant at a 1% level.\")"
      ],
      "metadata": {
        "id": "17SkrLCxi7aT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c3398c-f3b1-40ce-a38a-c4b76f6a5359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-Statistic: -0.4734\n",
            "P-Value: 0.6360\n",
            "Not significant at a 1% level.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X0 = mid_df['sentiment']\n",
        "X1 = dalle_df['sentiment']\n",
        "\n",
        "alpha = 0.01\n",
        "tstat, pval = stats.ttest_ind(X0,X1, equal_var = False)\n",
        "print(f'T-Statistic: {tstat:.4f}\\nP-Value: {pval:.4f}')\n",
        "if pval < alpha:\n",
        "    print(\"Significant at a 1% level.\")\n",
        "else:\n",
        "    print(\"Not significant at a 1% level.\")"
      ],
      "metadata": {
        "id": "BgoRVYzajA_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62668f91-552d-4946-e1af-352caae2d680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-Statistic: 7.1621\n",
            "P-Value: 0.0000\n",
            "Significant at a 1% level.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X0 = mid_df['sentiment']\n",
        "X1 = gpt_df['sentiment']\n",
        "\n",
        "alpha = 0.01\n",
        "tstat, pval = stats.ttest_ind(X0,X1, equal_var = False)\n",
        "print(f'T-Statistic: {tstat:.4f}\\nP-Value: {pval:.4f}')\n",
        "if pval < alpha:\n",
        "    print(\"Significant at a 1% level.\")\n",
        "else:\n",
        "    print(\"Not significant at a 1% level.\")"
      ],
      "metadata": {
        "id": "rkOttZuFjC_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77bb6e86-a953-4114-835e-25cf1e8a1dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-Statistic: 11.7391\n",
            "P-Value: 0.0000\n",
            "Significant at a 1% level.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_gpt_sentiment = gpt_df['sentiment'].mean()\n",
        "mean_dalle_sentiment = dalle_df['sentiment'].mean()\n",
        "mean_mid_sentiment = mid_df['sentiment'].mean()\n",
        "\n",
        "plt.bar(['ChatGPT', 'Dall-E', 'Midjourney'], [mean_gpt_sentiment, mean_dalle_sentiment, mean_mid_sentiment], color=['red', 'blue', 'green'])\n",
        "\n",
        "plt.xlabel('Keywords')\n",
        "plt.ylabel('Mean of Sentiment')\n",
        "plt.title('Mean of Tweet Sentiment by Keywords')\n",
        "\n",
        "plt.ylim(bottom=3.10, top=3.30)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Y8a0orqje1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Engagement Analysis"
      ],
      "metadata": {
        "id": "OY5h5X6WjLP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_df['like_engagement'] = gpt_df.like_count/(gpt_df.impression_count+1)\n",
        "gpt_df['retweet_engagement'] = gpt_df.retweet_count/(gpt_df.impression_count+1)\n",
        "gpt_df['reply_engagement'] = gpt_df.reply_count/(gpt_df.impression_count+1)\n",
        "\n",
        "dalle_df['like_engagement'] = dalle_df.like_count/(dalle_df.impression_count+1)\n",
        "dalle_df['retweet_engagement'] = dalle_df.retweet_count/(dalle_df.impression_count+1)\n",
        "dalle_df['reply_engagement'] = dalle_df.reply_count/(dalle_df.impression_count+1)\n",
        "\n",
        "mid_df['like_engagement'] = mid_df.like_count/(df.impression_count+1)\n",
        "mid_df['retweet_engagement'] = mid_df.retweet_count/(df.impression_count+1)\n",
        "mid_df['reply_engagement'] = mid_df.reply_count/(df.impression_count+1)"
      ],
      "metadata": {
        "id": "J2WiAdPNjJMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X0 = dalle_df['like_engagement']\n",
        "X1 = gpt_df['like_engagement']\n",
        "\n",
        "alpha = 0.01\n",
        "tstat, pval = stats.ttest_ind(X0,X1, equal_var = False)\n",
        "print(f'T-Statistic: {tstat:.4f}\\nP-Value: {pval:.4f}')\n",
        "if pval < alpha:\n",
        "    print(\"Significant at a 1% level.\")\n",
        "else:\n",
        "    print(\"Not significant at a 1% level.\")"
      ],
      "metadata": {
        "id": "pCD-ZdrRjPU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72848363-c5f4-4f1c-aa62-0fe3525e25d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-Statistic: 2.8102\n",
            "P-Value: 0.0050\n",
            "Significant at a 1% level.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X0 = dalle_df['retweet_engagement']\n",
        "X1 = gpt_df['retweet_engagement']\n",
        "\n",
        "alpha = 0.01\n",
        "tstat, pval = stats.ttest_ind(X0,X1, equal_var = False)\n",
        "print(f'T-Statistic: {tstat:.4f}\\nP-Value: {pval:.4f}')\n",
        "if pval < alpha:\n",
        "    print(\"Significant at a 1% level.\")\n",
        "else:\n",
        "    print(\"Not significant at a 1% level.\")"
      ],
      "metadata": {
        "id": "j72pNyUGjRrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dea7a4a-642e-4ace-8948-75e3dadc88c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-Statistic: -1.8001\n",
            "P-Value: 0.0719\n",
            "Not significant at a 1% level.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X0 = dalle_df['reply_engagement']\n",
        "X1 = gpt_df['reply_engagement']\n",
        "\n",
        "alpha = 0.01\n",
        "tstat, pval = stats.ttest_ind(X0,X1, equal_var = False)\n",
        "print(f'T-Statistic: {tstat:.4f}\\nP-Value: {pval:.4f}')\n",
        "if pval < alpha:\n",
        "    print(\"Significant at a 1% level.\")\n",
        "else:\n",
        "    print(\"Not significant at a 1% level.\")"
      ],
      "metadata": {
        "id": "aN7TTWChjTpq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1664dd-e296-4888-c17e-41ec3ba3f357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-Statistic: -2.6797\n",
            "P-Value: 0.0074\n",
            "Significant at a 1% level.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_gpt_like = gpt_df['like_engagement'].mean()\n",
        "mean_dalle_like = dalle_df['like_engagement'].mean()\n",
        "mean_mid_like = mid_df['like_engagement'].mean()\n",
        "\n",
        "plt.bar(['ChatGPT', 'Dall-E', 'Midjourney'], [mean_gpt_like, mean_dalle_like, mean_mid_like], color=['red', 'blue', 'green'])\n",
        "\n",
        "plt.xlabel('Keywords')\n",
        "plt.ylabel('Mean of Like Engagement')\n",
        "plt.title('Mean of Like Engagement by Tweet Keywords')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0jsekSZjjXAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_gpt_retweet = gpt_df['retweet_engagement'].mean()\n",
        "mean_dalle_retweet = dalle_df['retweet_engagement'].mean()\n",
        "mean_mid_retweet = mid_df['retweet_engagement'].mean()\n",
        "\n",
        "plt.bar(['ChatGPT', 'Dall-E', 'Midjourney'], [mean_gpt_retweet, mean_dalle_retweet, mean_mid_retweet], color=['red', 'blue', 'green'])\n",
        "\n",
        "plt.xlabel('Keywords')\n",
        "plt.ylabel('Mean of Retweet Engagement')\n",
        "plt.title('Mean of Retweet Engagement by Tweet Keywords')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QHND7z2BjZMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_gpt_reply = gpt_df['reply_engagement'].mean()\n",
        "mean_dalle_reply = dalle_df['reply_engagement'].mean()\n",
        "mean_mid_reply = mid_df['reply_engagement'].mean()\n",
        "\n",
        "plt.bar(['ChatGPT', 'Dall-E', 'Midjourney'], [mean_gpt_reply, mean_dalle_reply, mean_mid_reply], color=['red', 'blue', 'green'])\n",
        "\n",
        "plt.xlabel('Keywords')\n",
        "plt.ylabel('Mean of Reply Engagement')\n",
        "plt.title('Mean of Reply Engagement by Tweet Keywords')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V0VZcqMKjbXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Centrality measurements"
      ],
      "metadata": {
        "id": "5cOlV6EvkQIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fname_db = f\"AItweets_v2\"\n",
        "df = DB.fetch(table_name='keyword_tweets',path=fname_db)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "r9wd0pZgkHRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_clean'] = df.text.apply(ta.clean_tweet)  #clean the tweets\n",
        "df = df[df.text_clean.str.len() >0]  #remove cleaned tweets of length 0\n",
        "nclean = len(df)\n",
        "print(f\" {nclean} clean tweets\")"
      ],
      "metadata": {
        "id": "Cmd68RMQkM-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = interaction_network_from_tweets(df)\n",
        "\n",
        "nv = G.number_of_nodes()\n",
        "ne = G.number_of_edges()\n",
        "print(f\"Network has {nv} nodes and {ne} edges\")"
      ],
      "metadata": {
        "id": "QjVIU_FgkWY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Comm = nx_comm.greedy_modularity_communities(G.to_undirected())\n",
        "C = []\n",
        "V = []\n",
        "for count,comm in enumerate(Comm):\n",
        "    for v in comm:\n",
        "        C.append(count)\n",
        "        V.append(v)\n",
        "\n",
        "df_mod = pd.DataFrame({'screen_name':V, 'community':C})\n",
        "df_mod.head()\n",
        "\n",
        "ncomm_mod = len(df_mod.community.unique())\n",
        "print(f\"{ncomm_mod} modularity communities\")"
      ],
      "metadata": {
        "id": "xZz1KNXhkYUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5\n",
        "df_spec = spectral_communities(G,k)\n",
        "\n",
        "ncomm_spec = len(df_spec.community.unique())\n",
        "print(f\"{ncomm_spec} spectral communities\")"
      ],
      "metadata": {
        "id": "tASIp9FckZHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot modularity community sizes\n",
        "fig = plt.figure(figsize = (16,6))\n",
        "plt.subplot(1,2,1)\n",
        "ax = sns.countplot(data=df_mod, x=\"community\")\n",
        "plt.xlabel(\"Community\", fontsize = 14)\n",
        "plt.ylabel(\"Number of nodes\", fontsize = 14)\n",
        "plt.title(f\"{ncomm_mod} modularity communities\", fontsize = 18)\n",
        "plt.xticks(fontsize = 12)\n",
        "plt.yticks(fontsize = 12)\n",
        "\n",
        "plt.grid()\n",
        "\n",
        "\n",
        "#plot spectral community sizes\n",
        "plt.subplot(1,2,2)\n",
        "ax = sns.countplot(data=df_spec, x=\"community\")\n",
        "plt.xlabel(\"Community\", fontsize = 14)\n",
        "plt.ylabel(\"Number of nodes\", fontsize = 14)\n",
        "plt.title(f\"{ncomm_spec} spectral communities\", fontsize = 18)\n",
        "plt.grid()\n",
        "plt.xticks(fontsize = 12)\n",
        "plt.yticks(fontsize = 12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6N5Qpm9wkg89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spec.groupby(by='community',as_index=False).count()"
      ],
      "metadata": {
        "id": "Uk3bjXSYkl_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mod_size = df_mod.groupby(by='community',as_index=False).count()\n",
        "df_mod_size = df_mod_size.rename(columns={\"screen_name\":'size'})\n",
        "size_min = df_mod_size[\"size\"].min()\n",
        "size_max =  df_mod_size[\"size\"].max()\n",
        "size_mean =  df_mod_size[\"size\"].mean()\n",
        "\n",
        "print(f\"Minimum community size = {size_min:.3f} nodes\")\n",
        "print(f\"Maximum community size = {size_max:.3f} nodes\")\n",
        "print(f\"Mean community size = {size_mean:.3f} nodes\")"
      ],
      "metadata": {
        "id": "fCI41IWtkmHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Din = nx.in_degree_centrality(G)\n",
        "Dout = nx.out_degree_centrality(G)\n",
        "EC = nx.eigenvector_centrality(G.reverse(),max_iter = 1000)  #reverse edges to match networx convention\n",
        "\n",
        "dictionary_list = []\n",
        "for author_id in Din.keys():\n",
        "    screen_name = G.nodes[author_id]['username']\n",
        "    row = {'author_id':author_id,\n",
        "           'screen_name':screen_name,\n",
        "          'out_degree_centrality':Dout[author_id],\n",
        "          'eigenvector_centrality':EC[author_id],\n",
        "          'in_degree_centrality':Din[author_id]}\n",
        "    dictionary_list.append(row)\n",
        "df_centrality = pd.DataFrame(dictionary_list)"
      ],
      "metadata": {
        "id": "LJg7pbZCkokO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centrality = \"eigenvector_centrality\"\n",
        "nmax = 10  #number of top screen names to display\n",
        "df_centrality.sort_values(by = [centrality],ascending = False)[['screen_name',centrality]].head(n=nmax)"
      ],
      "metadata": {
        "id": "TBwVVuLrkq3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_centrality.head()"
      ],
      "metadata": {
        "id": "TmlFlfnXku-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Centrality_names = df_centrality.columns.tolist()[2:]\n",
        "nmax = 10  #number of top screen names to plot\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize = (20,14))\n",
        "\n",
        "for count,centrality_name in enumerate(Centrality_names):\n",
        "    df_plot = df_centrality.sort_values(by=[centrality_name],ascending=False)  #sort dataframe by centrality value\n",
        "    plt.subplot(2,3,count+1) #make a 2 x 3 subplot, plot in box cnt+1\n",
        "\n",
        "    ax = sns.barplot(data = df_plot[0:nmax],\n",
        "                     x='screen_name',\n",
        "                     y=centrality_name)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\n",
        "    plt.ylabel(f\"{centrality_name}\")\n",
        "    plt.xlabel('Screen name')\n",
        "    plt.grid()\n",
        "\n",
        "plt.subplots_adjust(left=None,\n",
        "                    bottom=None,\n",
        "                    right=None,\n",
        "                    top=None,\n",
        "                    wspace=None,\n",
        "                    hspace=0.55)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "10lvapCXkxdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Keyword analysis for AI tweets"
      ],
      "metadata": {
        "id": "rPMRagu8r47o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load up tweets for analysis"
      ],
      "metadata": {
        "id": "zm8ugGTasDsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fname_db = '/content/drive/MyDrive/Social Media Analytics/AItweets_v2'\n",
        "df = DB.fetch(table_name = \"keyword_tweets\", path = fname_db)\n",
        "# df = df[[\"screen_name\", \"text\", \"retweet_count\"]]\n",
        "n = len(df)\n",
        "print(f\"\\n {n} tweets originally\")\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "8L6gY7Crr6mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove duplicates"
      ],
      "metadata": {
        "id": "H71sY0DisTYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# n = len(df)\n",
        "print(f\"{n} tweets originally\")\n",
        "\n",
        "df = df.drop_duplicates(subset='id').copy()\n",
        "\n",
        "n = len(df)\n",
        "print(f\"\\n {n} tweets after removing duplicates\")"
      ],
      "metadata": {
        "id": "t1m2DulGsQQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean text, remove bots"
      ],
      "metadata": {
        "id": "3xhdwEuesVsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_clean'] = df.text.apply(ta.clean_tweet)  #clean the tweets\n",
        "df = df[df.text_clean.str.len() >0]  #remove cleaned tweets of length 0\n",
        "nclean = len(df)\n",
        "print(f\"{n} tweets, {nclean} clean tweets\")\n",
        "\n",
        "# df.sample(n=2)\n",
        "\n",
        "# found this to be most likely a bot activity, definitely spam\n",
        "# df = df[~df['text_clean'].str.contains('')]\n",
        "\n",
        "df = df[~df['text_clean'].str.contains('health consultations we are revolutionizing the way people access medical')]\n",
        "df = df[~df['text_clean'].str.contains('ai revolution download to participate in the ai2earn economy')]\n",
        "df = df[~df['text_clean'].str.contains('total maximum supply only 10 units a great artificial intelligence project dont miss')]\n",
        "df = df[~df['text_clean'].str.contains('blockchain based virtual world that allows users to create build buy and sell')]\n",
        "df = df[~df['text_clean'].str.contains('neurobayes fairlaunch starts at')]\n",
        "df = df[~df['text_clean'].str.contains('live happily togesssa')]"
      ],
      "metadata": {
        "id": "LCVeXp6nsYwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter for AI, Artificial Intelligence tweets from larger dataset into new df"
      ],
      "metadata": {
        "id": "Jlg_82udse0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[f'keyword_artificial'] = df.text_clean.str.contains('artificial intelligence', case=False)\n",
        "df[f'keyword_ai'] = df.text_clean.str.contains('AI', case=False)\n",
        "\n",
        "print(f\"'Artificial Intelligence': {len(df[df[f'keyword_artificial']==True])} tweets \\n\")\n",
        "print(f\"'AI': {len(df[df[f'keyword_ai']==True])} tweets \")\n",
        "\n",
        "df_ai = df[(df['keyword_ai'] == True) | (df['keyword_artificial'] == True)].copy()"
      ],
      "metadata": {
        "id": "PQ3pCZ4nsmlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Engagement scores"
      ],
      "metadata": {
        "id": "7eFnOWM3tfL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ai['like_engagement'] = df_ai.like_count/(df_ai.impression_count+1)\n",
        "df_ai['retweet_engagement'] = df_ai.retweet_count/(df_ai.impression_count+1)\n",
        "df_ai['reply_engagement'] = df_ai.reply_count/(df_ai.impression_count+1)\n",
        "\n",
        "df_ai = df_ai[df_ai.like_engagement<=1]\n",
        "df_ai = df_ai[df_ai.retweet_engagement<=1]\n",
        "df_ai = df_ai[df_ai.reply_engagement<=1]\n",
        "\n",
        "print(f\"{len(df_ai)} total tweets\")"
      ],
      "metadata": {
        "id": "eqyqOY34t4wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify and track keywords for engagement"
      ],
      "metadata": {
        "id": "ZHkvCKTUt-W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keywords2 = ['elon', 'economy', 'health', 'gpt', 'data', 'revolution', 'prompt', 'midjourney', 'artist', 'google']\n",
        "df_ai_copy = df_ai\n",
        "\n",
        "for keyword in keywords2:\n",
        "  df_ai_copy[f'keyword_{keyword}'] = df_ai_copy.text_clean.str.contains(keyword, case=False)\n",
        "  print(f\"\\n {keyword}: {len(df_ai_copy[df_ai_copy[f'keyword_{keyword}']==True])} tweets\")\n",
        "\n",
        "print('\\n')\n",
        "df_ai_copy.sample(n=2)"
      ],
      "metadata": {
        "id": "qTMwLtkPt73-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run t-tests"
      ],
      "metadata": {
        "id": "MTVEMsFRuLPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Like engagement analysis"
      ],
      "metadata": {
        "id": "K-KVUTQkuVRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for keyword in keywords2:\n",
        "  X0 = df_ai_copy.like_engagement[df_ai_copy[f\"keyword_{keyword}\"]==False].dropna().tolist()\n",
        "  X1 = df_ai_copy.like_engagement[df_ai_copy[f\"keyword_{keyword}\"]==True].dropna().tolist()\n",
        "\n",
        "  n0 = len(X0)\n",
        "  n1 = len(X1)\n",
        "  mu0 = np.mean(X0)\n",
        "  mu1 = np.mean(X1)\n",
        "\n",
        "  print(f\"\\n\\nKeyword: {keyword}\\nPresent? \\tNumber of tweets\\tMean like engagement\")\n",
        "  print(f\"False\\t\\t{n0}\\t\\t\\t{mu0:.3f}\")\n",
        "  print(f\"True\\t\\t{n1}\\t\\t\\t{mu1:.3f}\")\n",
        "\n",
        "  (tstat, pval) =stats.ttest_ind(X0,X1, equal_var = False)\n",
        "\n",
        "  print('T-test')\n",
        "  print(f\"{keyword}: t-stat = {tstat:.3f} ({pval:.3f})\\n\")\n",
        "  alpha = 0.01  #significance level\n",
        "  if pval <=alpha:\n",
        "    print(\"Significant at 1% level\\n-----------------\")\n",
        "  else:\n",
        "    print(\"Not significant at 1% level\\n-----------------\")"
      ],
      "metadata": {
        "id": "abw2wzxLuIYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retweet engagement"
      ],
      "metadata": {
        "id": "j5TK9so0uYpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for keyword in keywords2:\n",
        "  X0 = df_ai_copy.retweet_engagement[df_ai_copy[f\"keyword_{keyword}\"]==False].dropna().tolist()\n",
        "  X1 = df_ai_copy.retweet_engagement[df_ai_copy[f\"keyword_{keyword}\"]==True].dropna().tolist()\n",
        "\n",
        "  n0 = len(X0)\n",
        "  n1 = len(X1)\n",
        "  mu0 = np.mean(X0)\n",
        "  mu1 = np.mean(X1)\n",
        "\n",
        "  print(f\"\\n\\nKeyword: {keyword}\\nPresent? \\tNumber of tweets\\tMean retweet engagement\")\n",
        "  print(f\"False\\t\\t{n0}\\t\\t\\t{mu0:.3f}\")\n",
        "  print(f\"True\\t\\t{n1}\\t\\t\\t{mu1:.3f}\")\n",
        "\n",
        "  (tstat, pval) =stats.ttest_ind(X0,X1, equal_var = False)\n",
        "\n",
        "  print('T-test')\n",
        "  print(f\"{keyword}: t-stat = {tstat:.3f} ({pval:.3f})\\n\")\n",
        "  alpha = 0.01  #significance level\n",
        "  if pval <=alpha:\n",
        "    print(\"Significant at 1% level\\n-----------------\")\n",
        "  else:\n",
        "    print(\"Not significant at 1% level\\n-----------------\")"
      ],
      "metadata": {
        "id": "ybWs35HUuet4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reply engagement"
      ],
      "metadata": {
        "id": "rvVNobCVuh6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for keyword in keywords2:\n",
        "  X0 = df_ai_copy.reply_engagement[df_ai_copy[f\"keyword_{keyword}\"]==False].dropna().tolist()\n",
        "  X1 = df_ai_copy.reply_engagement[df_ai_copy[f\"keyword_{keyword}\"]==True].dropna().tolist()\n",
        "\n",
        "  n0 = len(X0)\n",
        "  n1 = len(X1)\n",
        "  mu0 = np.mean(X0)\n",
        "  mu1 = np.mean(X1)\n",
        "\n",
        "  print(f\"\\n\\nKeyword: {keyword}\\nPresent? \\tNumber of tweets\\tMean reply engagement\")\n",
        "  print(f\"False\\t\\t{n0}\\t\\t\\t{mu0:.3f}\")\n",
        "  print(f\"True\\t\\t{n1}\\t\\t\\t{mu1:.3f}\")\n",
        "\n",
        "  (tstat, pval) =stats.ttest_ind(X0,X1, equal_var = False)\n",
        "\n",
        "  print('T-test')\n",
        "  print(f\"{keyword}: t-stat = {tstat:.3f} ({pval:.3f})\\n\")\n",
        "  alpha = 0.01  #significance level\n",
        "  if pval <=alpha:\n",
        "    print(\"Significant at 1% level\\n-----------------\")\n",
        "  else:\n",
        "    print(\"Not significant at 1% level\\n-----------------\")"
      ],
      "metadata": {
        "id": "-cH20WEWujpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html '/content/social_media_analytics/SMA_Project_Team6.ipynb'"
      ],
      "metadata": {
        "id": "jPwoogTToxIP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}